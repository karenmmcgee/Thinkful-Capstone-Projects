{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Capstone Project\n",
    "# Goal: Build an unsupervised model that will classify authors based on the style of writing using natural language processing.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions to clean, lemmatize and process the text file as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function expects a raw string. The purpose of this function is to take the string\n",
    "#text and remove double dashes, punctuactions and special characters.\n",
    "def text_cleaner(text):\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(r'_','',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    #text = re.sub(r'[\\r|\\n|\\r\\n]+', \"\", text)\n",
    "    text = re.sub('[^a-zA-z\\s]', \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to lemmatize the list\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "#nlp = spacy.load('en', parse = False, tag=False, entity=False)\n",
    "\n",
    "\n",
    "#create a function to lemmatize the text in the combine list\n",
    "def lemmatizestring(text):\n",
    "\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "\n",
    "    return(\" \".join([lmtzr.lemmatize(word) for word in word_tokenize(text)]))\n",
    "    \n",
    "    #using the following line of code creates a [E088] Text of length 1166183 exceeds maximum of 1000000.\n",
    "    #text = nlp(text)\n",
    "    #text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    #return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The function will take a raw text string and tokenize the words for given number. \n",
    "#For example, if the function is given a raw text string and the 1000 as input parameters\n",
    "#it will create 1000 tokenized words store it in a list and repeat the process. The result \n",
    "#will be multiple lists equal to the num_of_words parametar.\n",
    "#The purpose of this function is create a list of words that can be passed as input parameters \n",
    "#to our tfidf function that is used to create featurs.\n",
    "\n",
    "def chunck_words_old(text,num_of_words):\n",
    "    \n",
    "    text = text.split(' ') #split on whitespace creates a token\n",
    "    results = []\n",
    "    res = []\n",
    "    for word in text:\n",
    "        res.append(word)\n",
    "        if len(res)==num_of_words:\n",
    "            results.append(\" \".join(res))\n",
    "            res = []\n",
    "    results.append(\" \".join(res))\n",
    "    return(results) \n",
    "\n",
    "def chunck_words(text,num_of_words,source):\n",
    "    text = text.split(' ') #split on whitespace creates a token\n",
    "    results = []\n",
    "    res = []\n",
    "    author = []\n",
    "    for word in text: #take 1000 words of text\n",
    "        res.append(word) #add the words to a list one by one\n",
    "        if len(res)==num_of_words: # When there are a total of 100 words\n",
    "            results.append(\" \".join(res)) #join the words together into a string by removing the white space\n",
    "            author.append(source)\n",
    "            res = [] #clear the list\n",
    "    results.append(\" \".join(res)) #print the final string with all of the words together\n",
    "    author.append(source)\n",
    "    return(results,author)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import text files in string format from ten different authors/sources. Execute functions to clean, lemmatize and process the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "#find a title for the newbook\n",
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in the documents from nltk in a raw string \n",
    "#gutenberg.raw\n",
    "#:return: the given file(s) as a single string.\n",
    "#:rtype: str\n",
    "caesar_raw_string = gutenberg.raw('shakespeare-caesar.txt')\n",
    "hamlet_raw_string = gutenberg.raw('shakespeare-hamlet.txt')\n",
    "macbeth_raw_string = gutenberg.raw('shakespeare-macbeth.txt')\n",
    "ball_raw_string = gutenberg.raw('chesterton-ball.txt')\n",
    "brown_raw_string = gutenberg.raw('chesterton-brown.txt')\n",
    "thursday_raw_string = gutenberg.raw('chesterton-thursday.txt')\n",
    "moby_dick_raw_string = gutenberg.raw('melville-moby_dick.txt')\n",
    "bryant_raw_string = gutenberg.raw('bryant-stories.txt')\n",
    "leaves_raw_string = gutenberg.raw('whitman-leaves.txt')\n",
    "busterbrown_raw_string = gutenberg.raw('burgess-busterbrown.txt')\n",
    "blake_raw_string = gutenberg.raw('blake-poems.txt')\n",
    "bible_raw_string = gutenberg.raw('bible-kjv.txt')\n",
    "alice_raw_string = gutenberg.raw('carroll-alice.txt')\n",
    "persuasion_raw_string = gutenberg.raw('austen-persuasion.txt')\n",
    "sense_raw_string = gutenberg.raw('austen-sense.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify output\n",
    "#macbeth_raw_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean the raw file loaded as a string and print the output\n",
    "caesar_cleaned_text = text_cleaner(caesar_raw_string)\n",
    "hamlet_cleaned_text = text_cleaner(hamlet_raw_string)\n",
    "macbeth_cleaned_text = text_cleaner(macbeth_raw_string)\n",
    "ball_cleaned_text = text_cleaner(ball_raw_string)\n",
    "brown_cleaned_text = text_cleaner(brown_raw_string)\n",
    "thursday_cleaned_text = text_cleaner(thursday_raw_string)\n",
    "moby_dick_cleaned_text = text_cleaner(moby_dick_raw_string)\n",
    "bryant_cleaned_text = text_cleaner(bryant_raw_string)\n",
    "leaves_cleaned_text = text_cleaner(leaves_raw_string)\n",
    "busterbrown_cleaned_text = text_cleaner(busterbrown_raw_string)\n",
    "blake_cleaned_text = text_cleaner(blake_raw_string)\n",
    "bible_cleaned_text = text_cleaner(bible_raw_string)\n",
    "alice_cleaned_text = text_cleaner(alice_raw_string)\n",
    "persuasion_cleaned_text = text_cleaner(persuasion_raw_string)\n",
    "sense_cleaned_text = text_cleaner(sense_raw_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify output\n",
    "#macbeth_cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatize the file to show the root words\n",
    "caesar_lemmatize_text = lemmatizestring(caesar_cleaned_text)\n",
    "hamlet_lemmatize_text = lemmatizestring(hamlet_cleaned_text)\n",
    "macbeth_lemmatize_text = lemmatizestring(macbeth_cleaned_text)\n",
    "ball_lemmatize_text = lemmatizestring(ball_cleaned_text)\n",
    "brown_lemmatize_text = lemmatizestring(brown_cleaned_text)\n",
    "thursday_lemmatize_text = lemmatizestring(thursday_cleaned_text)\n",
    "moby_dick_lemmatize_text = lemmatizestring(moby_dick_cleaned_text)\n",
    "bryant_lemmatize_text = lemmatizestring(bryant_cleaned_text)\n",
    "leaves_lemmatize_text = lemmatizestring(leaves_cleaned_text)\n",
    "busterbrown_lemmatize_text = lemmatizestring(busterbrown_cleaned_text)\n",
    "blake_lemmatize_text = lemmatizestring(blake_cleaned_text)\n",
    "bible_lemmatize_text = lemmatizestring(bible_cleaned_text)\n",
    "alice_lemmatize_text = lemmatizestring(alice_cleaned_text)\n",
    "persuasion_lemmatize_text = lemmatizestring(persuasion_cleaned_text)\n",
    "sense_lemmatize_text = lemmatizestring(sense_cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take the raw string and tokenize it for a default of 1000 words. However, decrease the \n",
    "#the num_of_words increases the size of the file. The opposite (increate of num_of_words\n",
    "#decreases the size of the file). There to ensure the file size of each file is similar\n",
    "#I am adjusting the value of num_of_words for each file.\n",
    "\n",
    "num_of_words = 1000 #use for default value\n",
    "new_tokenized_list_caesar,author0=chunck_words(caesar_lemmatize_text,200,'shakespeare')\n",
    "new_tokenized_list_hamlet,author1=chunck_words(hamlet_lemmatize_text,200,'shakespeare')\n",
    "new_tokenized_list_macbeth,author2=chunck_words(macbeth_lemmatize_text,200,'shakespeare')\n",
    "new_tokenized_list_ball,author3=chunck_words(ball_lemmatize_text,550,'chesterton')\n",
    "new_tokenized_list_brown,author4=chunck_words(brown_lemmatize_text,500,'chesterton')\n",
    "new_tokenized_list_thursday,author5=chunck_words(thursday_lemmatize_text,500,'chesterton')\n",
    "new_tokenized_list_moby_dick,author6=chunck_words(moby_dick_lemmatize_text,num_of_words,'melville')\n",
    "new_tokenized_list_bryant,author7=chunck_words(bryant_lemmatize_text,200,'bryant')\n",
    "new_tokenized_list_leaves,author8=chunck_words(leaves_lemmatize_text,450,'whitman')\n",
    "new_tokenized_list_busterbrown,author9=chunck_words(busterbrown_lemmatize_text,100,'burgess')\n",
    "new_tokenized_list_blake,author10=chunck_words(blake_lemmatize_text,50,'blake')\n",
    "new_tokenized_list_bible,author11=chunck_words(bible_lemmatize_text,3000,'bible')\n",
    "new_tokenized_list_alice,author12=chunck_words(alice_lemmatize_text,100,'carroll')\n",
    "new_tokenized_list_persuasion,author13=chunck_words(persuasion_lemmatize_text,300,'austen')\n",
    "new_tokenized_list_sense,author14=chunck_words(sense_lemmatize_text,num_of_words,'austen')\n",
    "\n",
    "#new_tokenized_list_ball,author3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify output\n",
    "#new_tokenized_list_macbeth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After all files have been imported, cleaned and process then identify the lenght of each file and combine them into one file. Create a new dataframe that contains the aurthor/source and document text of each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenght of caesar plus author:  102 102\n",
      "lenght of hamlet plus author:  148 148\n",
      "lenght of macbeth plus author:  89 89\n",
      "lenght of ball plus author:  149 149\n",
      "lenght of brown plus author:  144 144\n",
      "lenght of thursday plus author:  117 117\n",
      "lenght of moby dick plus author:  214 214\n",
      "lenght of bryant plus author:  231 231\n",
      "lenght of leaves plus author:  272 272\n",
      "lenght of busterbrown plus author:  159 159\n",
      "lenght of blake plus author:  137 137\n",
      "lenght of bible plus author:  264 264\n",
      "lenght of alice plus author:  266 266\n",
      "lenght of persuasion plus author:  278 278\n",
      "lenght of sense plus author:  120 120\n",
      "total lenght is 5380\n"
     ]
    }
   ],
   "source": [
    "#Print the lengths of the tokenized list, as this will help us later on verify if we \n",
    "#have the correct number of rows when we combine the files.\n",
    "\n",
    "#import unittest from TestStringMethods\n",
    "#unittest.assertEqual(new_tokenized_list_caesar,author0)\n",
    "\n",
    "print('lenght of caesar plus author: ',len(new_tokenized_list_caesar),len(author0))\n",
    "print('lenght of hamlet plus author: ',len(new_tokenized_list_hamlet),len(author1))\n",
    "print('lenght of macbeth plus author: ',len(new_tokenized_list_macbeth),len(author2))\n",
    "print('lenght of ball plus author: ',len(new_tokenized_list_ball),len(author3))\n",
    "print('lenght of brown plus author: ',len(new_tokenized_list_brown),len(author4))\n",
    "print('lenght of thursday plus author: ',len(new_tokenized_list_thursday),len(author5))\n",
    "print('lenght of moby dick plus author: ',len(new_tokenized_list_moby_dick),len(author6))\n",
    "print('lenght of bryant plus author: ',len(new_tokenized_list_bryant),len(author7))\n",
    "print('lenght of leaves plus author: ',len(new_tokenized_list_leaves),len(author8))\n",
    "print('lenght of busterbrown plus author: ',len(new_tokenized_list_busterbrown),len(author9))\n",
    "print('lenght of blake plus author: ',len(new_tokenized_list_blake),len(author10))\n",
    "print('lenght of bible plus author: ',len(new_tokenized_list_bible),len(author11))\n",
    "print('lenght of alice plus author: ',len(new_tokenized_list_alice),len(author12))\n",
    "print('lenght of persuasion plus author: ',len(new_tokenized_list_persuasion),len(author13))\n",
    "print('lenght of sense plus author: ',len(new_tokenized_list_sense),len(author14))\n",
    "\n",
    "print('total lenght is',len(new_tokenized_list_caesar)+len(author0)+\n",
    "     len(new_tokenized_list_hamlet)+len(author1)+ len(new_tokenized_list_macbeth)+len(author2)+\n",
    "     len(new_tokenized_list_ball)+len(author3)+len(new_tokenized_list_brown)+ len(author4)+\n",
    "     len(new_tokenized_list_thursday)+len(author5)+len(new_tokenized_list_moby_dick)+len(author6)+\n",
    "     len(new_tokenized_list_bryant)+len(author7)+len(new_tokenized_list_leaves)+len(author8)+\n",
    "     len(new_tokenized_list_busterbrown)+len(author9)+ len(new_tokenized_list_blake)+len(author10)+\n",
    "     len(new_tokenized_list_bible)+len(author11)+ len(new_tokenized_list_alice)+len(author12)+\n",
    "     len(new_tokenized_list_persuasion)+len(author13)+ len(new_tokenized_list_sense)+len(author14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combine tokenized length and author list: 5380\n"
     ]
    }
   ],
   "source": [
    "#Combine the list of tokenized files into one list \n",
    "combine_tokenized_liststring = new_tokenized_list_caesar \n",
    "combine_tokenized_liststring.extend(new_tokenized_list_hamlet)\n",
    "combine_tokenized_liststring.extend(new_tokenized_list_macbeth)\n",
    "combine_tokenized_liststring.extend(new_tokenized_list_ball)\n",
    "combine_tokenized_liststring.extend(new_tokenized_list_brown)\n",
    "combine_tokenized_liststring.extend(new_tokenized_list_thursday)   \n",
    "combine_tokenized_liststring.extend(new_tokenized_list_moby_dick)\n",
    "combine_tokenized_liststring.extend(new_tokenized_list_bryant)                              \n",
    "combine_tokenized_liststring.extend(new_tokenized_list_leaves)\n",
    "combine_tokenized_liststring.extend(new_tokenized_list_busterbrown)\n",
    "combine_tokenized_liststring.extend(new_tokenized_list_blake)\n",
    "combine_tokenized_liststring.extend(new_tokenized_list_bible)\n",
    "combine_tokenized_liststring.extend(new_tokenized_list_alice)\n",
    "combine_tokenized_liststring.extend(new_tokenized_list_persuasion)\n",
    "combine_tokenized_liststring.extend(new_tokenized_list_sense)\n",
    "\n",
    "#Combine the list of authors into one list\n",
    "combine_author_list= author0\n",
    "combine_author_list.extend(author1)\n",
    "combine_author_list.extend(author2)\n",
    "combine_author_list.extend(author3)\n",
    "combine_author_list.extend(author4)\n",
    "combine_author_list.extend(author5)\n",
    "combine_author_list.extend(author6)\n",
    "combine_author_list.extend(author7)\n",
    "combine_author_list.extend(author8)\n",
    "combine_author_list.extend(author9)\n",
    "combine_author_list.extend(author10)\n",
    "combine_author_list.extend(author11)\n",
    "combine_author_list.extend(author12)\n",
    "combine_author_list.extend(author13)\n",
    "combine_author_list.extend(author14)\n",
    "print('combine tokenized length and author list:',len(combine_tokenized_liststring)+len( combine_author_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze the dataframe and show the output of authors, document text and author codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataframe for the corpus texts and authors\n",
    "text_string_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Actus Primus Scoena Prima Enter Flauius Murell...</td>\n",
       "      <td>shakespeare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>matter nor woman matter but withal I am indeed...</td>\n",
       "      <td>shakespeare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>replication of your sound Made in her Concaue ...</td>\n",
       "      <td>shakespeare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wing Will make him flye an ordinary pitch Who ...</td>\n",
       "      <td>shakespeare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the throng look vpon Caesar Caes What sayst th...</td>\n",
       "      <td>shakespeare</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text      authors\n",
       "0  Actus Primus Scoena Prima Enter Flauius Murell...  shakespeare\n",
       "1  matter nor woman matter but withal I am indeed...  shakespeare\n",
       "2  replication of your sound Made in her Concaue ...  shakespeare\n",
       "3  wing Will make him flye an ordinary pitch Who ...  shakespeare\n",
       "4  the throng look vpon Caesar Caes What sayst th...  shakespeare"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add the combine list and author list as columns for the dataframe and view output\n",
    "text_string_df['text'] = combine_tokenized_liststring \n",
    "text_string_df['authors'] = combine_author_list\n",
    "text_string_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify pd series index\n",
    "#pd.Series.sort_index(text_string_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>authors</th>\n",
       "      <th>author_codes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Actus Primus Scoena Prima Enter Flauius Murell...</td>\n",
       "      <td>shakespeare</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>matter nor woman matter but withal I am indeed...</td>\n",
       "      <td>shakespeare</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>replication of your sound Made in her Concaue ...</td>\n",
       "      <td>shakespeare</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wing Will make him flye an ordinary pitch Who ...</td>\n",
       "      <td>shakespeare</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the throng look vpon Caesar Caes What sayst th...</td>\n",
       "      <td>shakespeare</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text      authors  \\\n",
       "0  Actus Primus Scoena Prima Enter Flauius Murell...  shakespeare   \n",
       "1  matter nor woman matter but withal I am indeed...  shakespeare   \n",
       "2  replication of your sound Made in her Concaue ...  shakespeare   \n",
       "3  wing Will make him flye an ordinary pitch Who ...  shakespeare   \n",
       "4  the throng look vpon Caesar Caes What sayst th...  shakespeare   \n",
       "\n",
       "   author_codes  \n",
       "0             8  \n",
       "1             8  \n",
       "2             8  \n",
       "3             8  \n",
       "4             8  "
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Make a column with an encoded label for each author\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb_make = LabelEncoder()\n",
    "text_string_df['author_codes'] = lb_make.fit_transform(text_string_df['authors'])\n",
    "text_string_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author_codes  authors    \n",
       "0             austen         398\n",
       "1             bible          264\n",
       "2             blake          137\n",
       "3             bryant         231\n",
       "4             burgess        159\n",
       "5             carroll        266\n",
       "6             chesterton     410\n",
       "7             melville       214\n",
       "8             shakespeare    339\n",
       "9             whitman        272\n",
       "Name: authors, dtype: int64"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#text_string_df['author_codes'].value_counts\n",
    "\n",
    "#print(text_string_df['authors'].value_counts())\n",
    "#print(text_string_df['author_codes'].value_counts())\n",
    "\n",
    "text_string_df.groupby('author_codes')['authors'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pos=np.arange(len(list(text_string_df.groupby('author_codes')['authors'].value_counts())))\n",
    "plt.barh(y_pos,text_string_df.groupby('author_codes')['authors'].value_counts(), align='center', alpha=0.5)\n",
    "plt.yticks(y_pos, text_string_df.groupby('author_codes')['authors'].unique())\n",
    "plt.ylabel('Authors')\n",
    "plt.title('Number of Documents by Author') \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining my X and Y variables\n",
    "X = text_string_df.text\n",
    "Y = text_string_df.author_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nltk's English stopwords as variable called 'stopwords' we will use this as a parameter\n",
    "# in the CountVectorizer and TFIDF function\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaron</th>\n",
       "      <th>aarons</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandonment</th>\n",
       "      <th>abase</th>\n",
       "      <th>abased</th>\n",
       "      <th>abated</th>\n",
       "      <th>abba</th>\n",
       "      <th>abbreviation</th>\n",
       "      <th>abdon</th>\n",
       "      <th>abel</th>\n",
       "      <th>abelmeholah</th>\n",
       "      <th>abhor</th>\n",
       "      <th>abhorred</th>\n",
       "      <th>abhorrence</th>\n",
       "      <th>abhorreth</th>\n",
       "      <th>abia</th>\n",
       "      <th>abiah</th>\n",
       "      <th>abiathar</th>\n",
       "      <th>abidan</th>\n",
       "      <th>abide</th>\n",
       "      <th>abideth</th>\n",
       "      <th>abiding</th>\n",
       "      <th>abiel</th>\n",
       "      <th>abiezer</th>\n",
       "      <th>abigail</th>\n",
       "      <th>abihail</th>\n",
       "      <th>abihu</th>\n",
       "      <th>abijah</th>\n",
       "      <th>ability</th>\n",
       "      <th>abimelech</th>\n",
       "      <th>abinadab</th>\n",
       "      <th>abiram</th>\n",
       "      <th>abishai</th>\n",
       "      <th>ablaze</th>\n",
       "      <th>able</th>\n",
       "      <th>abner</th>\n",
       "      <th>aboard</th>\n",
       "      <th>abode</th>\n",
       "      <th>abolished</th>\n",
       "      <th>abominable</th>\n",
       "      <th>abomination</th>\n",
       "      <th>aboriginal</th>\n",
       "      <th>aboue</th>\n",
       "      <th>abound</th>\n",
       "      <th>abounded</th>\n",
       "      <th>abounding</th>\n",
       "      <th>abraham</th>\n",
       "      <th>abrahams</th>\n",
       "      <th>abram</th>\n",
       "      <th>abreast</th>\n",
       "      <th>abroad</th>\n",
       "      <th>abrupt</th>\n",
       "      <th>abruptly</th>\n",
       "      <th>absalom</th>\n",
       "      <th>absence</th>\n",
       "      <th>absent</th>\n",
       "      <th>absently</th>\n",
       "      <th>absentminded</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>absorb</th>\n",
       "      <th>absorbed</th>\n",
       "      <th>absorbing</th>\n",
       "      <th>abstain</th>\n",
       "      <th>abstinence</th>\n",
       "      <th>abstract</th>\n",
       "      <th>abstracted</th>\n",
       "      <th>abstraction</th>\n",
       "      <th>absurd</th>\n",
       "      <th>absurdity</th>\n",
       "      <th>abundance</th>\n",
       "      <th>abundant</th>\n",
       "      <th>abundantly</th>\n",
       "      <th>abuse</th>\n",
       "      <th>abused</th>\n",
       "      <th>abyss</th>\n",
       "      <th>academy</th>\n",
       "      <th>accelerate</th>\n",
       "      <th>accent</th>\n",
       "      <th>accept</th>\n",
       "      <th>acceptable</th>\n",
       "      <th>acceptance</th>\n",
       "      <th>accepted</th>\n",
       "      <th>accepteth</th>\n",
       "      <th>accepting</th>\n",
       "      <th>accepts</th>\n",
       "      <th>access</th>\n",
       "      <th>accident</th>\n",
       "      <th>accidental</th>\n",
       "      <th>accidentally</th>\n",
       "      <th>accommodate</th>\n",
       "      <th>accommodation</th>\n",
       "      <th>accompanied</th>\n",
       "      <th>accompaniment</th>\n",
       "      <th>accompany</th>\n",
       "      <th>accompanying</th>\n",
       "      <th>accomplish</th>\n",
       "      <th>accomplishd</th>\n",
       "      <th>accomplished</th>\n",
       "      <th>accomplishment</th>\n",
       "      <th>accord</th>\n",
       "      <th>accordance</th>\n",
       "      <th>according</th>\n",
       "      <th>accordingly</th>\n",
       "      <th>accosted</th>\n",
       "      <th>account</th>\n",
       "      <th>accounted</th>\n",
       "      <th>accumulated</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accurately</th>\n",
       "      <th>accursed</th>\n",
       "      <th>accusation</th>\n",
       "      <th>accuse</th>\n",
       "      <th>accused</th>\n",
       "      <th>accuser</th>\n",
       "      <th>accustomary</th>\n",
       "      <th>accustomed</th>\n",
       "      <th>achaia</th>\n",
       "      <th>achbor</th>\n",
       "      <th>ache</th>\n",
       "      <th>achieve</th>\n",
       "      <th>achieved</th>\n",
       "      <th>achilles</th>\n",
       "      <th>aching</th>\n",
       "      <th>achish</th>\n",
       "      <th>acknowledge</th>\n",
       "      <th>acknowledged</th>\n",
       "      <th>acknowledges</th>\n",
       "      <th>acknowledging</th>\n",
       "      <th>acknowledgment</th>\n",
       "      <th>acorn</th>\n",
       "      <th>acquaintance</th>\n",
       "      <th>acquainted</th>\n",
       "      <th>acquiescence</th>\n",
       "      <th>acquired</th>\n",
       "      <th>acquit</th>\n",
       "      <th>acquitted</th>\n",
       "      <th>acre</th>\n",
       "      <th>across</th>\n",
       "      <th>act</th>\n",
       "      <th>acte</th>\n",
       "      <th>acted</th>\n",
       "      <th>acting</th>\n",
       "      <th>action</th>\n",
       "      <th>active</th>\n",
       "      <th>actively</th>\n",
       "      <th>activity</th>\n",
       "      <th>actor</th>\n",
       "      <th>actress</th>\n",
       "      <th>acts</th>\n",
       "      <th>actual</th>\n",
       "      <th>actually</th>\n",
       "      <th>actus</th>\n",
       "      <th>acute</th>\n",
       "      <th>acuteness</th>\n",
       "      <th>ad</th>\n",
       "      <th>adaiah</th>\n",
       "      <th>adam</th>\n",
       "      <th>adamant</th>\n",
       "      <th>adapted</th>\n",
       "      <th>add</th>\n",
       "      <th>added</th>\n",
       "      <th>adder</th>\n",
       "      <th>addeth</th>\n",
       "      <th>adding</th>\n",
       "      <th>addition</th>\n",
       "      <th>additional</th>\n",
       "      <th>address</th>\n",
       "      <th>addressed</th>\n",
       "      <th>addressing</th>\n",
       "      <th>adequately</th>\n",
       "      <th>adhere</th>\n",
       "      <th>adhesiveness</th>\n",
       "      <th>adiel</th>\n",
       "      <th>adieu</th>\n",
       "      <th>adin</th>\n",
       "      <th>adjoining</th>\n",
       "      <th>adjure</th>\n",
       "      <th>admah</th>\n",
       "      <th>admirable</th>\n",
       "      <th>admirably</th>\n",
       "      <th>admiral</th>\n",
       "      <th>admirals</th>\n",
       "      <th>admiration</th>\n",
       "      <th>admire</th>\n",
       "      <th>admired</th>\n",
       "      <th>admirer</th>\n",
       "      <th>admiring</th>\n",
       "      <th>admit</th>\n",
       "      <th>admittance</th>\n",
       "      <th>admitted</th>\n",
       "      <th>admitting</th>\n",
       "      <th>admonished</th>\n",
       "      <th>admonition</th>\n",
       "      <th>ado</th>\n",
       "      <th>adobie</th>\n",
       "      <th>adonijah</th>\n",
       "      <th>adopt</th>\n",
       "      <th>adopted</th>\n",
       "      <th>adoption</th>\n",
       "      <th>adoration</th>\n",
       "      <th>adorn</th>\n",
       "      <th>adorned</th>\n",
       "      <th>adrift</th>\n",
       "      <th>adullam</th>\n",
       "      <th>adulterer</th>\n",
       "      <th>adulteress</th>\n",
       "      <th>adulterous</th>\n",
       "      <th>adultery</th>\n",
       "      <th>advance</th>\n",
       "      <th>advanced</th>\n",
       "      <th>advancing</th>\n",
       "      <th>advantage</th>\n",
       "      <th>advantageous</th>\n",
       "      <th>adventure</th>\n",
       "      <th>adventurer</th>\n",
       "      <th>adventures</th>\n",
       "      <th>adventurous</th>\n",
       "      <th>adversary</th>\n",
       "      <th>adverse</th>\n",
       "      <th>adversity</th>\n",
       "      <th>advertise</th>\n",
       "      <th>advice</th>\n",
       "      <th>advisable</th>\n",
       "      <th>advise</th>\n",
       "      <th>advised</th>\n",
       "      <th>adviser</th>\n",
       "      <th>advocate</th>\n",
       "      <th>aeneas</th>\n",
       "      <th>afar</th>\n",
       "      <th>affair</th>\n",
       "      <th>affaire</th>\n",
       "      <th>affayres</th>\n",
       "      <th>affeard</th>\n",
       "      <th>affect</th>\n",
       "      <th>affectation</th>\n",
       "      <th>affected</th>\n",
       "      <th>affecting</th>\n",
       "      <th>affection</th>\n",
       "      <th>affectionate</th>\n",
       "      <th>affectionately</th>\n",
       "      <th>affirm</th>\n",
       "      <th>affirmed</th>\n",
       "      <th>afflict</th>\n",
       "      <th>afflicted</th>\n",
       "      <th>affliction</th>\n",
       "      <th>affluence</th>\n",
       "      <th>afford</th>\n",
       "      <th>...</th>\n",
       "      <th>wisconsin</th>\n",
       "      <th>wisdom</th>\n",
       "      <th>wise</th>\n",
       "      <th>wisedome</th>\n",
       "      <th>wisely</th>\n",
       "      <th>wiser</th>\n",
       "      <th>wisest</th>\n",
       "      <th>wish</th>\n",
       "      <th>wished</th>\n",
       "      <th>wishing</th>\n",
       "      <th>wist</th>\n",
       "      <th>wistful</th>\n",
       "      <th>wistfully</th>\n",
       "      <th>wit</th>\n",
       "      <th>witch</th>\n",
       "      <th>witchcraft</th>\n",
       "      <th>witches</th>\n",
       "      <th>withal</th>\n",
       "      <th>withall</th>\n",
       "      <th>withdraw</th>\n",
       "      <th>withdrawal</th>\n",
       "      <th>withdrawing</th>\n",
       "      <th>withdrawn</th>\n",
       "      <th>withdrew</th>\n",
       "      <th>wither</th>\n",
       "      <th>witherd</th>\n",
       "      <th>withered</th>\n",
       "      <th>withereth</th>\n",
       "      <th>withheld</th>\n",
       "      <th>withhold</th>\n",
       "      <th>withholden</th>\n",
       "      <th>within</th>\n",
       "      <th>without</th>\n",
       "      <th>withstand</th>\n",
       "      <th>withstood</th>\n",
       "      <th>witness</th>\n",
       "      <th>witnesse</th>\n",
       "      <th>witnessed</th>\n",
       "      <th>witnessing</th>\n",
       "      <th>witty</th>\n",
       "      <th>wizard</th>\n",
       "      <th>woe</th>\n",
       "      <th>woeful</th>\n",
       "      <th>woke</th>\n",
       "      <th>wolf</th>\n",
       "      <th>woman</th>\n",
       "      <th>womanhood</th>\n",
       "      <th>womb</th>\n",
       "      <th>women</th>\n",
       "      <th>wonder</th>\n",
       "      <th>wondered</th>\n",
       "      <th>wonderful</th>\n",
       "      <th>wonderfully</th>\n",
       "      <th>wondering</th>\n",
       "      <th>wonderment</th>\n",
       "      <th>wondrous</th>\n",
       "      <th>wont</th>\n",
       "      <th>wonted</th>\n",
       "      <th>wood</th>\n",
       "      <th>wooded</th>\n",
       "      <th>wooden</th>\n",
       "      <th>woodland</th>\n",
       "      <th>woods</th>\n",
       "      <th>woof</th>\n",
       "      <th>wool</th>\n",
       "      <th>woollen</th>\n",
       "      <th>word</th>\n",
       "      <th>wordless</th>\n",
       "      <th>words</th>\n",
       "      <th>wore</th>\n",
       "      <th>work</th>\n",
       "      <th>worke</th>\n",
       "      <th>worked</th>\n",
       "      <th>worker</th>\n",
       "      <th>worketh</th>\n",
       "      <th>working</th>\n",
       "      <th>workman</th>\n",
       "      <th>workmanlike</th>\n",
       "      <th>workmanship</th>\n",
       "      <th>workshop</th>\n",
       "      <th>world</th>\n",
       "      <th>worldly</th>\n",
       "      <th>worlds</th>\n",
       "      <th>worm</th>\n",
       "      <th>worms</th>\n",
       "      <th>wormwood</th>\n",
       "      <th>worn</th>\n",
       "      <th>worried</th>\n",
       "      <th>worry</th>\n",
       "      <th>worse</th>\n",
       "      <th>worship</th>\n",
       "      <th>worshipped</th>\n",
       "      <th>worshipper</th>\n",
       "      <th>worshippeth</th>\n",
       "      <th>worshipping</th>\n",
       "      <th>worst</th>\n",
       "      <th>worsted</th>\n",
       "      <th>worth</th>\n",
       "      <th>worthless</th>\n",
       "      <th>worthy</th>\n",
       "      <th>wot</th>\n",
       "      <th>would</th>\n",
       "      <th>wouldest</th>\n",
       "      <th>wouldnt</th>\n",
       "      <th>wouldst</th>\n",
       "      <th>wound</th>\n",
       "      <th>wounded</th>\n",
       "      <th>wounding</th>\n",
       "      <th>woven</th>\n",
       "      <th>wrap</th>\n",
       "      <th>wrapped</th>\n",
       "      <th>wrapping</th>\n",
       "      <th>wrapt</th>\n",
       "      <th>wrath</th>\n",
       "      <th>wreath</th>\n",
       "      <th>wreck</th>\n",
       "      <th>wreckd</th>\n",
       "      <th>wrecked</th>\n",
       "      <th>wren</th>\n",
       "      <th>wrench</th>\n",
       "      <th>wrenched</th>\n",
       "      <th>wrenching</th>\n",
       "      <th>wrest</th>\n",
       "      <th>wrestle</th>\n",
       "      <th>wrestled</th>\n",
       "      <th>wrestling</th>\n",
       "      <th>wretch</th>\n",
       "      <th>wretched</th>\n",
       "      <th>wretchedly</th>\n",
       "      <th>wretchedness</th>\n",
       "      <th>wriggle</th>\n",
       "      <th>wriggling</th>\n",
       "      <th>wring</th>\n",
       "      <th>wringing</th>\n",
       "      <th>wrinkle</th>\n",
       "      <th>wrinkled</th>\n",
       "      <th>wrist</th>\n",
       "      <th>writ</th>\n",
       "      <th>write</th>\n",
       "      <th>writer</th>\n",
       "      <th>writes</th>\n",
       "      <th>writhing</th>\n",
       "      <th>writing</th>\n",
       "      <th>written</th>\n",
       "      <th>wrong</th>\n",
       "      <th>wrongd</th>\n",
       "      <th>wronged</th>\n",
       "      <th>wrongfully</th>\n",
       "      <th>wrongly</th>\n",
       "      <th>wrote</th>\n",
       "      <th>wroth</th>\n",
       "      <th>wrought</th>\n",
       "      <th>wrung</th>\n",
       "      <th>xi</th>\n",
       "      <th>xiii</th>\n",
       "      <th>xiv</th>\n",
       "      <th>xv</th>\n",
       "      <th>xvi</th>\n",
       "      <th>yacht</th>\n",
       "      <th>yankee</th>\n",
       "      <th>yard</th>\n",
       "      <th>yardarm</th>\n",
       "      <th>yarn</th>\n",
       "      <th>yawned</th>\n",
       "      <th>yawning</th>\n",
       "      <th>ye</th>\n",
       "      <th>yea</th>\n",
       "      <th>year</th>\n",
       "      <th>yeare</th>\n",
       "      <th>yeares</th>\n",
       "      <th>yearly</th>\n",
       "      <th>yearning</th>\n",
       "      <th>years</th>\n",
       "      <th>yeeld</th>\n",
       "      <th>yell</th>\n",
       "      <th>yelled</th>\n",
       "      <th>yelling</th>\n",
       "      <th>yellow</th>\n",
       "      <th>yer</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yesternight</th>\n",
       "      <th>yet</th>\n",
       "      <th>yeve</th>\n",
       "      <th>yield</th>\n",
       "      <th>yielded</th>\n",
       "      <th>yieldeth</th>\n",
       "      <th>yielding</th>\n",
       "      <th>yojo</th>\n",
       "      <th>yoke</th>\n",
       "      <th>yoked</th>\n",
       "      <th>yon</th>\n",
       "      <th>yond</th>\n",
       "      <th>yonder</th>\n",
       "      <th>yong</th>\n",
       "      <th>york</th>\n",
       "      <th>yosemite</th>\n",
       "      <th>youd</th>\n",
       "      <th>youl</th>\n",
       "      <th>youll</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>youngest</th>\n",
       "      <th>youngster</th>\n",
       "      <th>youre</th>\n",
       "      <th>youth</th>\n",
       "      <th>youthful</th>\n",
       "      <th>youve</th>\n",
       "      <th>youward</th>\n",
       "      <th>zabad</th>\n",
       "      <th>zabdi</th>\n",
       "      <th>zaccur</th>\n",
       "      <th>zacharias</th>\n",
       "      <th>zadok</th>\n",
       "      <th>zanoah</th>\n",
       "      <th>zeal</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zealous</th>\n",
       "      <th>zealously</th>\n",
       "      <th>zebadiah</th>\n",
       "      <th>zebedee</th>\n",
       "      <th>zeboim</th>\n",
       "      <th>zebulun</th>\n",
       "      <th>zechariah</th>\n",
       "      <th>zedekiah</th>\n",
       "      <th>zelia</th>\n",
       "      <th>zephaniah</th>\n",
       "      <th>zerah</th>\n",
       "      <th>zerubbabel</th>\n",
       "      <th>zeruiah</th>\n",
       "      <th>zichri</th>\n",
       "      <th>zidon</th>\n",
       "      <th>zidonians</th>\n",
       "      <th>ziha</th>\n",
       "      <th>ziklag</th>\n",
       "      <th>zimri</th>\n",
       "      <th>zin</th>\n",
       "      <th>zion</th>\n",
       "      <th>ziph</th>\n",
       "      <th>zippor</th>\n",
       "      <th>zoan</th>\n",
       "      <th>zoar</th>\n",
       "      <th>zobah</th>\n",
       "      <th>zodiac</th>\n",
       "      <th>zohar</th>\n",
       "      <th>zone</th>\n",
       "      <th>zorah</th>\n",
       "      <th>zuar</th>\n",
       "      <th>zuph</th>\n",
       "      <th>zurishaddai</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.107995</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.122238</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068781</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.069472</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.114615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.11615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.048426</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060554</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043245</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.067442</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.085157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058584</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.057079</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.122451</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052886</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.045372</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071127</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  14011 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaron  aarons  abandon  abandoned  abandonment  abase  abased  abated  \\\n",
       "0    0.0     0.0      0.0        0.0          0.0    0.0     0.0     0.0   \n",
       "1    0.0     0.0      0.0        0.0          0.0    0.0     0.0     0.0   \n",
       "2    0.0     0.0      0.0        0.0          0.0    0.0     0.0     0.0   \n",
       "3    0.0     0.0      0.0        0.0          0.0    0.0     0.0     0.0   \n",
       "4    0.0     0.0      0.0        0.0          0.0    0.0     0.0     0.0   \n",
       "\n",
       "   abba  abbreviation  abdon  abel  abelmeholah  abhor  abhorred  abhorrence  \\\n",
       "0   0.0           0.0    0.0   0.0          0.0    0.0       0.0         0.0   \n",
       "1   0.0           0.0    0.0   0.0          0.0    0.0       0.0         0.0   \n",
       "2   0.0           0.0    0.0   0.0          0.0    0.0       0.0         0.0   \n",
       "3   0.0           0.0    0.0   0.0          0.0    0.0       0.0         0.0   \n",
       "4   0.0           0.0    0.0   0.0          0.0    0.0       0.0         0.0   \n",
       "\n",
       "   abhorreth  abia  abiah  abiathar  abidan  abide  abideth  abiding  abiel  \\\n",
       "0        0.0   0.0    0.0       0.0     0.0    0.0      0.0      0.0    0.0   \n",
       "1        0.0   0.0    0.0       0.0     0.0    0.0      0.0      0.0    0.0   \n",
       "2        0.0   0.0    0.0       0.0     0.0    0.0      0.0      0.0    0.0   \n",
       "3        0.0   0.0    0.0       0.0     0.0    0.0      0.0      0.0    0.0   \n",
       "4        0.0   0.0    0.0       0.0     0.0    0.0      0.0      0.0    0.0   \n",
       "\n",
       "   abiezer  abigail  abihail  abihu  abijah  ability  abimelech  abinadab  \\\n",
       "0      0.0      0.0      0.0    0.0     0.0      0.0        0.0       0.0   \n",
       "1      0.0      0.0      0.0    0.0     0.0      0.0        0.0       0.0   \n",
       "2      0.0      0.0      0.0    0.0     0.0      0.0        0.0       0.0   \n",
       "3      0.0      0.0      0.0    0.0     0.0      0.0        0.0       0.0   \n",
       "4      0.0      0.0      0.0    0.0     0.0      0.0        0.0       0.0   \n",
       "\n",
       "   abiram  abishai  ablaze  able  abner  aboard  abode  abolished  abominable  \\\n",
       "0     0.0      0.0     0.0   0.0    0.0     0.0    0.0        0.0         0.0   \n",
       "1     0.0      0.0     0.0   0.0    0.0     0.0    0.0        0.0         0.0   \n",
       "2     0.0      0.0     0.0   0.0    0.0     0.0    0.0        0.0         0.0   \n",
       "3     0.0      0.0     0.0   0.0    0.0     0.0    0.0        0.0         0.0   \n",
       "4     0.0      0.0     0.0   0.0    0.0     0.0    0.0        0.0         0.0   \n",
       "\n",
       "   abomination  aboriginal  aboue  abound  abounded  abounding  abraham  \\\n",
       "0          0.0         0.0    0.0     0.0       0.0        0.0      0.0   \n",
       "1          0.0         0.0    0.0     0.0       0.0        0.0      0.0   \n",
       "2          0.0         0.0    0.0     0.0       0.0        0.0      0.0   \n",
       "3          0.0         0.0    0.0     0.0       0.0        0.0      0.0   \n",
       "4          0.0         0.0    0.0     0.0       0.0        0.0      0.0   \n",
       "\n",
       "   abrahams  abram  abreast  abroad    abrupt  abruptly  absalom  absence  \\\n",
       "0       0.0    0.0      0.0     0.0  0.000000       0.0      0.0      0.0   \n",
       "1       0.0    0.0      0.0     0.0  0.069472       0.0      0.0      0.0   \n",
       "2       0.0    0.0      0.0     0.0  0.000000       0.0      0.0      0.0   \n",
       "3       0.0    0.0      0.0     0.0  0.000000       0.0      0.0      0.0   \n",
       "4       0.0    0.0      0.0     0.0  0.000000       0.0      0.0      0.0   \n",
       "\n",
       "   absent  absently  absentminded  absolute  absolutely  absorb  absorbed  \\\n",
       "0  0.1052       0.0           0.0       0.0         0.0     0.0       0.0   \n",
       "1  0.0000       0.0           0.0       0.0         0.0     0.0       0.0   \n",
       "2  0.0000       0.0           0.0       0.0         0.0     0.0       0.0   \n",
       "3  0.0000       0.0           0.0       0.0         0.0     0.0       0.0   \n",
       "4  0.0000       0.0           0.0       0.0         0.0     0.0       0.0   \n",
       "\n",
       "   absorbing  abstain  abstinence  abstract  abstracted  abstraction  absurd  \\\n",
       "0        0.0      0.0         0.0       0.0         0.0          0.0     0.0   \n",
       "1        0.0      0.0         0.0       0.0         0.0          0.0     0.0   \n",
       "2        0.0      0.0         0.0       0.0         0.0          0.0     0.0   \n",
       "3        0.0      0.0         0.0       0.0         0.0          0.0     0.0   \n",
       "4        0.0      0.0         0.0       0.0         0.0          0.0     0.0   \n",
       "\n",
       "   absurdity  abundance  abundant  abundantly  abuse  abused  abyss  academy  \\\n",
       "0        0.0        0.0       0.0         0.0    0.0     0.0    0.0      0.0   \n",
       "1        0.0        0.0       0.0         0.0    0.0     0.0    0.0      0.0   \n",
       "2        0.0        0.0       0.0         0.0    0.0     0.0    0.0      0.0   \n",
       "3        0.0        0.0       0.0         0.0    0.0     0.0    0.0      0.0   \n",
       "4        0.0        0.0       0.0         0.0    0.0     0.0    0.0      0.0   \n",
       "\n",
       "   accelerate  accent  accept  acceptable  acceptance  accepted  accepteth  \\\n",
       "0         0.0     0.0     0.0         0.0         0.0       0.0        0.0   \n",
       "1         0.0     0.0     0.0         0.0         0.0       0.0        0.0   \n",
       "2         0.0     0.0     0.0         0.0         0.0       0.0        0.0   \n",
       "3         0.0     0.0     0.0         0.0         0.0       0.0        0.0   \n",
       "4         0.0     0.0     0.0         0.0         0.0       0.0        0.0   \n",
       "\n",
       "   accepting  accepts  access  accident  accidental  accidentally  \\\n",
       "0        0.0      0.0     0.0       0.0         0.0           0.0   \n",
       "1        0.0      0.0     0.0       0.0         0.0           0.0   \n",
       "2        0.0      0.0     0.0       0.0         0.0           0.0   \n",
       "3        0.0      0.0     0.0       0.0         0.0           0.0   \n",
       "4        0.0      0.0     0.0       0.0         0.0           0.0   \n",
       "\n",
       "   accommodate  accommodation  accompanied  accompaniment  accompany  \\\n",
       "0          0.0            0.0          0.0            0.0        0.0   \n",
       "1          0.0            0.0          0.0            0.0        0.0   \n",
       "2          0.0            0.0          0.0            0.0        0.0   \n",
       "3          0.0            0.0          0.0            0.0        0.0   \n",
       "4          0.0            0.0          0.0            0.0        0.0   \n",
       "\n",
       "   accompanying  accomplish  accomplishd  accomplished  accomplishment  \\\n",
       "0           0.0         0.0          0.0           0.0             0.0   \n",
       "1           0.0         0.0          0.0           0.0             0.0   \n",
       "2           0.0         0.0          0.0           0.0             0.0   \n",
       "3           0.0         0.0          0.0           0.0             0.0   \n",
       "4           0.0         0.0          0.0           0.0             0.0   \n",
       "\n",
       "   accord  accordance  according  accordingly  accosted  account  accounted  \\\n",
       "0     0.0         0.0        0.0          0.0       0.0      0.0        0.0   \n",
       "1     0.0         0.0        0.0          0.0       0.0      0.0        0.0   \n",
       "2     0.0         0.0        0.0          0.0       0.0      0.0        0.0   \n",
       "3     0.0         0.0        0.0          0.0       0.0      0.0        0.0   \n",
       "4     0.0         0.0        0.0          0.0       0.0      0.0        0.0   \n",
       "\n",
       "   accumulated  accuracy  accurately  accursed  accusation  accuse  accused  \\\n",
       "0          0.0       0.0         0.0       0.0         0.0     0.0      0.0   \n",
       "1          0.0       0.0         0.0       0.0         0.0     0.0      0.0   \n",
       "2          0.0       0.0         0.0       0.0         0.0     0.0      0.0   \n",
       "3          0.0       0.0         0.0       0.0         0.0     0.0      0.0   \n",
       "4          0.0       0.0         0.0       0.0         0.0     0.0      0.0   \n",
       "\n",
       "   accuser  accustomary  accustomed  achaia  achbor  ache  achieve  achieved  \\\n",
       "0      0.0          0.0         0.0     0.0     0.0   0.0      0.0       0.0   \n",
       "1      0.0          0.0         0.0     0.0     0.0   0.0      0.0       0.0   \n",
       "2      0.0          0.0         0.0     0.0     0.0   0.0      0.0       0.0   \n",
       "3      0.0          0.0         0.0     0.0     0.0   0.0      0.0       0.0   \n",
       "4      0.0          0.0         0.0     0.0     0.0   0.0      0.0       0.0   \n",
       "\n",
       "   achilles  aching  achish  acknowledge  acknowledged  acknowledges  \\\n",
       "0       0.0     0.0     0.0          0.0           0.0           0.0   \n",
       "1       0.0     0.0     0.0          0.0           0.0           0.0   \n",
       "2       0.0     0.0     0.0          0.0           0.0           0.0   \n",
       "3       0.0     0.0     0.0          0.0           0.0           0.0   \n",
       "4       0.0     0.0     0.0          0.0           0.0           0.0   \n",
       "\n",
       "   acknowledging  acknowledgment  acorn  acquaintance  acquainted  \\\n",
       "0            0.0             0.0    0.0           0.0         0.0   \n",
       "1            0.0             0.0    0.0           0.0         0.0   \n",
       "2            0.0             0.0    0.0           0.0         0.0   \n",
       "3            0.0             0.0    0.0           0.0         0.0   \n",
       "4            0.0             0.0    0.0           0.0         0.0   \n",
       "\n",
       "   acquiescence  acquired  acquit  acquitted  acre    across  act  acte  \\\n",
       "0           0.0       0.0     0.0        0.0   0.0  0.000000  0.0   0.0   \n",
       "1           0.0       0.0     0.0        0.0   0.0  0.000000  0.0   0.0   \n",
       "2           0.0       0.0     0.0        0.0   0.0  0.000000  0.0   0.0   \n",
       "3           0.0       0.0     0.0        0.0   0.0  0.048426  0.0   0.0   \n",
       "4           0.0       0.0     0.0        0.0   0.0  0.000000  0.0   0.0   \n",
       "\n",
       "   acted  acting  action  active  actively  activity  actor  actress  acts  \\\n",
       "0    0.0     0.0     0.0     0.0       0.0       0.0    0.0      0.0   0.0   \n",
       "1    0.0     0.0     0.0     0.0       0.0       0.0    0.0      0.0   0.0   \n",
       "2    0.0     0.0     0.0     0.0       0.0       0.0    0.0      0.0   0.0   \n",
       "3    0.0     0.0     0.0     0.0       0.0       0.0    0.0      0.0   0.0   \n",
       "4    0.0     0.0     0.0     0.0       0.0       0.0    0.0      0.0   0.0   \n",
       "\n",
       "   actual  actually  actus  acute  acuteness   ad  adaiah  adam  adamant  \\\n",
       "0     0.0       0.0    0.0    0.0        0.0  0.0     0.0   0.0      0.0   \n",
       "1     0.0       0.0    0.0    0.0        0.0  0.0     0.0   0.0      0.0   \n",
       "2     0.0       0.0    0.0    0.0        0.0  0.0     0.0   0.0      0.0   \n",
       "3     0.0       0.0    0.0    0.0        0.0  0.0     0.0   0.0      0.0   \n",
       "4     0.0       0.0    0.0    0.0        0.0  0.0     0.0   0.0      0.0   \n",
       "\n",
       "   adapted  add  added  adder  addeth  adding  addition  additional  address  \\\n",
       "0      0.0  0.0    0.0    0.0     0.0     0.0       0.0         0.0      0.0   \n",
       "1      0.0  0.0    0.0    0.0     0.0     0.0       0.0         0.0      0.0   \n",
       "2      0.0  0.0    0.0    0.0     0.0     0.0       0.0         0.0      0.0   \n",
       "3      0.0  0.0    0.0    0.0     0.0     0.0       0.0         0.0      0.0   \n",
       "4      0.0  0.0    0.0    0.0     0.0     0.0       0.0         0.0      0.0   \n",
       "\n",
       "   addressed  addressing  adequately  adhere  adhesiveness  adiel  adieu  \\\n",
       "0        0.0         0.0         0.0     0.0           0.0    0.0    0.0   \n",
       "1        0.0         0.0         0.0     0.0           0.0    0.0    0.0   \n",
       "2        0.0         0.0         0.0     0.0           0.0    0.0    0.0   \n",
       "3        0.0         0.0         0.0     0.0           0.0    0.0    0.0   \n",
       "4        0.0         0.0         0.0     0.0           0.0    0.0    0.0   \n",
       "\n",
       "   adin  adjoining  adjure  admah  admirable  admirably   admiral  admirals  \\\n",
       "0   0.0        0.0     0.0    0.0        0.0        0.0  0.000000       0.0   \n",
       "1   0.0        0.0     0.0    0.0        0.0        0.0  0.000000       0.0   \n",
       "2   0.0        0.0     0.0    0.0        0.0        0.0  0.000000       0.0   \n",
       "3   0.0        0.0     0.0    0.0        0.0        0.0  0.060554       0.0   \n",
       "4   0.0        0.0     0.0    0.0        0.0        0.0  0.000000       0.0   \n",
       "\n",
       "   admiration    admire  admired  admirer  admiring  admit  admittance  \\\n",
       "0         0.0  0.107995      0.0      0.0       0.0    0.0         0.0   \n",
       "1         0.0  0.000000      0.0      0.0       0.0    0.0         0.0   \n",
       "2         0.0  0.000000      0.0      0.0       0.0    0.0         0.0   \n",
       "3         0.0  0.000000      0.0      0.0       0.0    0.0         0.0   \n",
       "4         0.0  0.000000      0.0      0.0       0.0    0.0         0.0   \n",
       "\n",
       "   admitted  admitting  admonished  admonition  ado  adobie  adonijah  adopt  \\\n",
       "0       0.0        0.0         0.0         0.0  0.0     0.0       0.0    0.0   \n",
       "1       0.0        0.0         0.0         0.0  0.0     0.0       0.0    0.0   \n",
       "2       0.0        0.0         0.0         0.0  0.0     0.0       0.0    0.0   \n",
       "3       0.0        0.0         0.0         0.0  0.0     0.0       0.0    0.0   \n",
       "4       0.0        0.0         0.0         0.0  0.0     0.0       0.0    0.0   \n",
       "\n",
       "    adopted  adoption  adoration  adorn  adorned  adrift  adullam  adulterer  \\\n",
       "0  0.000000       0.0        0.0    0.0      0.0     0.0      0.0        0.0   \n",
       "1  0.000000       0.0        0.0    0.0      0.0     0.0      0.0        0.0   \n",
       "2  0.000000       0.0        0.0    0.0      0.0     0.0      0.0        0.0   \n",
       "3  0.000000       0.0        0.0    0.0      0.0     0.0      0.0        0.0   \n",
       "4  0.122451       0.0        0.0    0.0      0.0     0.0      0.0        0.0   \n",
       "\n",
       "   adulteress  adulterous  adultery   advance  advanced  advancing  advantage  \\\n",
       "0         0.0         0.0       0.0  0.000000       0.0        0.0        0.0   \n",
       "1         0.0         0.0       0.0  0.114615       0.0        0.0        0.0   \n",
       "2         0.0         0.0       0.0  0.000000       0.0        0.0        0.0   \n",
       "3         0.0         0.0       0.0  0.000000       0.0        0.0        0.0   \n",
       "4         0.0         0.0       0.0  0.000000       0.0        0.0        0.0   \n",
       "\n",
       "   advantageous  adventure  adventurer  adventures  adventurous  adversary  \\\n",
       "0           0.0        0.0         0.0         0.0          0.0        0.0   \n",
       "1           0.0        0.0         0.0         0.0          0.0        0.0   \n",
       "2           0.0        0.0         0.0         0.0          0.0        0.0   \n",
       "3           0.0        0.0         0.0         0.0          0.0        0.0   \n",
       "4           0.0        0.0         0.0         0.0          0.0        0.0   \n",
       "\n",
       "   adverse  adversity  advertise  advice  advisable  advise  advised  adviser  \\\n",
       "0      0.0        0.0        0.0     0.0        0.0     0.0      0.0      0.0   \n",
       "1      0.0        0.0        0.0     0.0        0.0     0.0      0.0      0.0   \n",
       "2      0.0        0.0        0.0     0.0        0.0     0.0      0.0      0.0   \n",
       "3      0.0        0.0        0.0     0.0        0.0     0.0      0.0      0.0   \n",
       "4      0.0        0.0        0.0     0.0        0.0     0.0      0.0      0.0   \n",
       "\n",
       "   advocate  aeneas  afar  affair  affaire  affayres  affeard  affect  \\\n",
       "0       0.0     0.0   0.0     0.0      0.0       0.0      0.0     0.0   \n",
       "1       0.0     0.0   0.0     0.0      0.0       0.0      0.0     0.0   \n",
       "2       0.0     0.0   0.0     0.0      0.0       0.0      0.0     0.0   \n",
       "3       0.0     0.0   0.0     0.0      0.0       0.0      0.0     0.0   \n",
       "4       0.0     0.0   0.0     0.0      0.0       0.0      0.0     0.0   \n",
       "\n",
       "   affectation  affected  affecting  affection  affectionate  affectionately  \\\n",
       "0          0.0       0.0        0.0        0.0           0.0             0.0   \n",
       "1          0.0       0.0        0.0        0.0           0.0             0.0   \n",
       "2          0.0       0.0        0.0        0.0           0.0             0.0   \n",
       "3          0.0       0.0        0.0        0.0           0.0             0.0   \n",
       "4          0.0       0.0        0.0        0.0           0.0             0.0   \n",
       "\n",
       "   affirm  affirmed  afflict  afflicted  affliction  affluence  afford  \\\n",
       "0     0.0       0.0      0.0        0.0         0.0        0.0     0.0   \n",
       "1     0.0       0.0      0.0        0.0         0.0        0.0     0.0   \n",
       "2     0.0       0.0      0.0        0.0         0.0        0.0     0.0   \n",
       "3     0.0       0.0      0.0        0.0         0.0        0.0     0.0   \n",
       "4     0.0       0.0      0.0        0.0         0.0        0.0     0.0   \n",
       "\n",
       "      ...       wisconsin  wisdom  wise  wisedome  wisely  wiser  wisest  \\\n",
       "0     ...             0.0     0.0   0.0       0.0     0.0    0.0     0.0   \n",
       "1     ...             0.0     0.0   0.0       0.0     0.0    0.0     0.0   \n",
       "2     ...             0.0     0.0   0.0       0.0     0.0    0.0     0.0   \n",
       "3     ...             0.0     0.0   0.0       0.0     0.0    0.0     0.0   \n",
       "4     ...             0.0     0.0   0.0       0.0     0.0    0.0     0.0   \n",
       "\n",
       "   wish  wished  wishing  wist  wistful  wistfully  wit  witch  witchcraft  \\\n",
       "0   0.0     0.0      0.0   0.0      0.0        0.0  0.0    0.0         0.0   \n",
       "1   0.0     0.0      0.0   0.0      0.0        0.0  0.0    0.0         0.0   \n",
       "2   0.0     0.0      0.0   0.0      0.0        0.0  0.0    0.0         0.0   \n",
       "3   0.0     0.0      0.0   0.0      0.0        0.0  0.0    0.0         0.0   \n",
       "4   0.0     0.0      0.0   0.0      0.0        0.0  0.0    0.0         0.0   \n",
       "\n",
       "   witches  withal  withall  withdraw  withdrawal  withdrawing  withdrawn  \\\n",
       "0      0.0     0.0      0.0       0.0         0.0          0.0        0.0   \n",
       "1      0.0     0.0      0.0       0.0         0.0          0.0        0.0   \n",
       "2      0.0     0.0      0.0       0.0         0.0          0.0        0.0   \n",
       "3      0.0     0.0      0.0       0.0         0.0          0.0        0.0   \n",
       "4      0.0     0.0      0.0       0.0         0.0          0.0        0.0   \n",
       "\n",
       "   withdrew  wither  witherd  withered  withereth  withheld  withhold  \\\n",
       "0       0.0     0.0      0.0       0.0        0.0       0.0       0.0   \n",
       "1       0.0     0.0      0.0       0.0        0.0       0.0       0.0   \n",
       "2       0.0     0.0      0.0       0.0        0.0       0.0       0.0   \n",
       "3       0.0     0.0      0.0       0.0        0.0       0.0       0.0   \n",
       "4       0.0     0.0      0.0       0.0        0.0       0.0       0.0   \n",
       "\n",
       "   withholden  within  without  withstand  withstood  witness  witnesse  \\\n",
       "0         0.0     0.0      0.0        0.0        0.0      0.0       0.0   \n",
       "1         0.0     0.0      0.0        0.0        0.0      0.0       0.0   \n",
       "2         0.0     0.0      0.0        0.0        0.0      0.0       0.0   \n",
       "3         0.0     0.0      0.0        0.0        0.0      0.0       0.0   \n",
       "4         0.0     0.0      0.0        0.0        0.0      0.0       0.0   \n",
       "\n",
       "   witnessed  witnessing  witty  wizard  woe  woeful  woke  wolf     woman  \\\n",
       "0   0.122238         0.0    0.0     0.0  0.0     0.0   0.0   0.0  0.000000   \n",
       "1   0.000000         0.0    0.0     0.0  0.0     0.0   0.0   0.0  0.000000   \n",
       "2   0.000000         0.0    0.0     0.0  0.0     0.0   0.0   0.0  0.000000   \n",
       "3   0.000000         0.0    0.0     0.0  0.0     0.0   0.0   0.0  0.000000   \n",
       "4   0.000000         0.0    0.0     0.0  0.0     0.0   0.0   0.0  0.052886   \n",
       "\n",
       "   womanhood  womb  women  wonder  wondered  wonderful  wonderfully  \\\n",
       "0        0.0   0.0    0.0     0.0       0.0        0.0          0.0   \n",
       "1        0.0   0.0    0.0     0.0       0.0        0.0          0.0   \n",
       "2        0.0   0.0    0.0     0.0       0.0        0.0          0.0   \n",
       "3        0.0   0.0    0.0     0.0       0.0        0.0          0.0   \n",
       "4        0.0   0.0    0.0     0.0       0.0        0.0          0.0   \n",
       "\n",
       "   wondering  wonderment  wondrous  wont  wonted      wood  wooded  wooden  \\\n",
       "0        0.0         0.0   0.00000   0.0     0.0  0.000000     0.0     0.0   \n",
       "1        0.0         0.0   0.11615   0.0     0.0  0.041105     0.0     0.0   \n",
       "2        0.0         0.0   0.00000   0.0     0.0  0.000000     0.0     0.0   \n",
       "3        0.0         0.0   0.00000   0.0     0.0  0.043245     0.0     0.0   \n",
       "4        0.0         0.0   0.00000   0.0     0.0  0.000000     0.0     0.0   \n",
       "\n",
       "   woodland  woods  woof  wool  woollen      word  wordless  words  wore  \\\n",
       "0       0.0    0.0   0.0   0.0      0.0  0.000000       0.0    0.0   0.0   \n",
       "1       0.0    0.0   0.0   0.0      0.0  0.000000       0.0    0.0   0.0   \n",
       "2       0.0    0.0   0.0   0.0      0.0  0.000000       0.0    0.0   0.0   \n",
       "3       0.0    0.0   0.0   0.0      0.0  0.000000       0.0    0.0   0.0   \n",
       "4       0.0    0.0   0.0   0.0      0.0  0.045372       0.0    0.0   0.0   \n",
       "\n",
       "   work  worke  worked  worker  worketh  working  workman  workmanlike  \\\n",
       "0   0.0    0.0     0.0     0.0      0.0      0.0      0.0          0.0   \n",
       "1   0.0    0.0     0.0     0.0      0.0      0.0      0.0          0.0   \n",
       "2   0.0    0.0     0.0     0.0      0.0      0.0      0.0          0.0   \n",
       "3   0.0    0.0     0.0     0.0      0.0      0.0      0.0          0.0   \n",
       "4   0.0    0.0     0.0     0.0      0.0      0.0      0.0          0.0   \n",
       "\n",
       "   workmanship  workshop  world  worldly  worlds  worm  worms  wormwood  \\\n",
       "0          0.0       0.0    0.0      0.0     0.0   0.0    0.0       0.0   \n",
       "1          0.0       0.0    0.0      0.0     0.0   0.0    0.0       0.0   \n",
       "2          0.0       0.0    0.0      0.0     0.0   0.0    0.0       0.0   \n",
       "3          0.0       0.0    0.0      0.0     0.0   0.0    0.0       0.0   \n",
       "4          0.0       0.0    0.0      0.0     0.0   0.0    0.0       0.0   \n",
       "\n",
       "       worn  worried  worry  worse  worship  worshipped  worshipper  \\\n",
       "0  0.000000      0.0    0.0    0.0      0.0         0.0         0.0   \n",
       "1  0.000000      0.0    0.0    0.0      0.0         0.0         0.0   \n",
       "2  0.000000      0.0    0.0    0.0      0.0         0.0         0.0   \n",
       "3  0.067442      0.0    0.0    0.0      0.0         0.0         0.0   \n",
       "4  0.000000      0.0    0.0    0.0      0.0         0.0         0.0   \n",
       "\n",
       "   worshippeth  worshipping  worst  worsted  worth  worthless  worthy  wot  \\\n",
       "0          0.0          0.0    0.0      0.0    0.0        0.0     0.0  0.0   \n",
       "1          0.0          0.0    0.0      0.0    0.0        0.0     0.0  0.0   \n",
       "2          0.0          0.0    0.0      0.0    0.0        0.0     0.0  0.0   \n",
       "3          0.0          0.0    0.0      0.0    0.0        0.0     0.0  0.0   \n",
       "4          0.0          0.0    0.0      0.0    0.0        0.0     0.0  0.0   \n",
       "\n",
       "      would  wouldest  wouldnt  wouldst  wound  wounded  wounding  woven  \\\n",
       "0  0.068781       0.0      0.0      0.0    0.0      0.0       0.0    0.0   \n",
       "1  0.000000       0.0      0.0      0.0    0.0      0.0       0.0    0.0   \n",
       "2  0.000000       0.0      0.0      0.0    0.0      0.0       0.0    0.0   \n",
       "3  0.000000       0.0      0.0      0.0    0.0      0.0       0.0    0.0   \n",
       "4  0.071127       0.0      0.0      0.0    0.0      0.0       0.0    0.0   \n",
       "\n",
       "   wrap  wrapped  wrapping  wrapt  wrath  wreath  wreck  wreckd   wrecked  \\\n",
       "0   0.0      0.0       0.0    0.0    0.0     0.0    0.0     0.0  0.000000   \n",
       "1   0.0      0.0       0.0    0.0    0.0     0.0    0.0     0.0  0.000000   \n",
       "2   0.0      0.0       0.0    0.0    0.0     0.0    0.0     0.0  0.000000   \n",
       "3   0.0      0.0       0.0    0.0    0.0     0.0    0.0     0.0  0.085157   \n",
       "4   0.0      0.0       0.0    0.0    0.0     0.0    0.0     0.0  0.000000   \n",
       "\n",
       "   wren  wrench  wrenched  wrenching  wrest  wrestle  wrestled  wrestling  \\\n",
       "0   0.0     0.0       0.0        0.0    0.0      0.0       0.0        0.0   \n",
       "1   0.0     0.0       0.0        0.0    0.0      0.0       0.0        0.0   \n",
       "2   0.0     0.0       0.0        0.0    0.0      0.0       0.0        0.0   \n",
       "3   0.0     0.0       0.0        0.0    0.0      0.0       0.0        0.0   \n",
       "4   0.0     0.0       0.0        0.0    0.0      0.0       0.0        0.0   \n",
       "\n",
       "   wretch  wretched  wretchedly  wretchedness  wriggle  wriggling  wring  \\\n",
       "0     0.0       0.0         0.0           0.0      0.0        0.0    0.0   \n",
       "1     0.0       0.0         0.0           0.0      0.0        0.0    0.0   \n",
       "2     0.0       0.0         0.0           0.0      0.0        0.0    0.0   \n",
       "3     0.0       0.0         0.0           0.0      0.0        0.0    0.0   \n",
       "4     0.0       0.0         0.0           0.0      0.0        0.0    0.0   \n",
       "\n",
       "   wringing  wrinkle  wrinkled  wrist  writ  write  writer  writes  writhing  \\\n",
       "0       0.0      0.0       0.0    0.0   0.0    0.0     0.0     0.0       0.0   \n",
       "1       0.0      0.0       0.0    0.0   0.0    0.0     0.0     0.0       0.0   \n",
       "2       0.0      0.0       0.0    0.0   0.0    0.0     0.0     0.0       0.0   \n",
       "3       0.0      0.0       0.0    0.0   0.0    0.0     0.0     0.0       0.0   \n",
       "4       0.0      0.0       0.0    0.0   0.0    0.0     0.0     0.0       0.0   \n",
       "\n",
       "   writing  written  wrong  wrongd  wronged  wrongfully  wrongly  wrote  \\\n",
       "0      0.0      0.0    0.0     0.0      0.0         0.0      0.0    0.0   \n",
       "1      0.0      0.0    0.0     0.0      0.0         0.0      0.0    0.0   \n",
       "2      0.0      0.0    0.0     0.0      0.0         0.0      0.0    0.0   \n",
       "3      0.0      0.0    0.0     0.0      0.0         0.0      0.0    0.0   \n",
       "4      0.0      0.0    0.0     0.0      0.0         0.0      0.0    0.0   \n",
       "\n",
       "   wroth  wrought  wrung   xi  xiii  xiv   xv  xvi  yacht  yankee      yard  \\\n",
       "0    0.0      0.0    0.0  0.0   0.0  0.0  0.0  0.0    0.0     0.0  0.000000   \n",
       "1    0.0      0.0    0.0  0.0   0.0  0.0  0.0  0.0    0.0     0.0  0.000000   \n",
       "2    0.0      0.0    0.0  0.0   0.0  0.0  0.0  0.0    0.0     0.0  0.000000   \n",
       "3    0.0      0.0    0.0  0.0   0.0  0.0  0.0  0.0    0.0     0.0  0.058584   \n",
       "4    0.0      0.0    0.0  0.0   0.0  0.0  0.0  0.0    0.0     0.0  0.000000   \n",
       "\n",
       "   yardarm  yarn  yawned  yawning   ye  yea  year  yeare  yeares  yearly  \\\n",
       "0      0.0   0.0     0.0      0.0  0.0  0.0   0.0    0.0     0.0     0.0   \n",
       "1      0.0   0.0     0.0      0.0  0.0  0.0   0.0    0.0     0.0     0.0   \n",
       "2      0.0   0.0     0.0      0.0  0.0  0.0   0.0    0.0     0.0     0.0   \n",
       "3      0.0   0.0     0.0      0.0  0.0  0.0   0.0    0.0     0.0     0.0   \n",
       "4      0.0   0.0     0.0      0.0  0.0  0.0   0.0    0.0     0.0     0.0   \n",
       "\n",
       "   yearning  years  yeeld  yell  yelled  yelling    yellow  yer  yes  \\\n",
       "0       0.0    0.0    0.0   0.0     0.0      0.0  0.000000  0.0  0.0   \n",
       "1       0.0    0.0    0.0   0.0     0.0      0.0  0.000000  0.0  0.0   \n",
       "2       0.0    0.0    0.0   0.0     0.0      0.0  0.000000  0.0  0.0   \n",
       "3       0.0    0.0    0.0   0.0     0.0      0.0  0.057079  0.0  0.0   \n",
       "4       0.0    0.0    0.0   0.0     0.0      0.0  0.000000  0.0  0.0   \n",
       "\n",
       "   yesterday  yesternight  yet  yeve  yield  yielded  yieldeth  yielding  \\\n",
       "0        0.0          0.0  0.0   0.0    0.0      0.0       0.0       0.0   \n",
       "1        0.0          0.0  0.0   0.0    0.0      0.0       0.0       0.0   \n",
       "2        0.0          0.0  0.0   0.0    0.0      0.0       0.0       0.0   \n",
       "3        0.0          0.0  0.0   0.0    0.0      0.0       0.0       0.0   \n",
       "4        0.0          0.0  0.0   0.0    0.0      0.0       0.0       0.0   \n",
       "\n",
       "   yojo  yoke  yoked  yon  yond  yonder  yong  york  yosemite  youd  youl  \\\n",
       "0   0.0   0.0    0.0  0.0   0.0     0.0   0.0   0.0       0.0   0.0   0.0   \n",
       "1   0.0   0.0    0.0  0.0   0.0     0.0   0.0   0.0       0.0   0.0   0.0   \n",
       "2   0.0   0.0    0.0  0.0   0.0     0.0   0.0   0.0       0.0   0.0   0.0   \n",
       "3   0.0   0.0    0.0  0.0   0.0     0.0   0.0   0.0       0.0   0.0   0.0   \n",
       "4   0.0   0.0    0.0  0.0   0.0     0.0   0.0   0.0       0.0   0.0   0.0   \n",
       "\n",
       "   youll  young  younger  youngest  youngster  youre  youth  youthful  youve  \\\n",
       "0    0.0    0.0      0.0       0.0        0.0    0.0    0.0       0.0    0.0   \n",
       "1    0.0    0.0      0.0       0.0        0.0    0.0    0.0       0.0    0.0   \n",
       "2    0.0    0.0      0.0       0.0        0.0    0.0    0.0       0.0    0.0   \n",
       "3    0.0    0.0      0.0       0.0        0.0    0.0    0.0       0.0    0.0   \n",
       "4    0.0    0.0      0.0       0.0        0.0    0.0    0.0       0.0    0.0   \n",
       "\n",
       "   youward  zabad  zabdi  zaccur  zacharias  zadok  zanoah  zeal  zealand  \\\n",
       "0      0.0    0.0    0.0     0.0        0.0    0.0     0.0   0.0      0.0   \n",
       "1      0.0    0.0    0.0     0.0        0.0    0.0     0.0   0.0      0.0   \n",
       "2      0.0    0.0    0.0     0.0        0.0    0.0     0.0   0.0      0.0   \n",
       "3      0.0    0.0    0.0     0.0        0.0    0.0     0.0   0.0      0.0   \n",
       "4      0.0    0.0    0.0     0.0        0.0    0.0     0.0   0.0      0.0   \n",
       "\n",
       "   zealous  zealously  zebadiah  zebedee  zeboim  zebulun  zechariah  \\\n",
       "0      0.0        0.0       0.0      0.0     0.0      0.0        0.0   \n",
       "1      0.0        0.0       0.0      0.0     0.0      0.0        0.0   \n",
       "2      0.0        0.0       0.0      0.0     0.0      0.0        0.0   \n",
       "3      0.0        0.0       0.0      0.0     0.0      0.0        0.0   \n",
       "4      0.0        0.0       0.0      0.0     0.0      0.0        0.0   \n",
       "\n",
       "   zedekiah  zelia  zephaniah  zerah  zerubbabel  zeruiah  zichri  zidon  \\\n",
       "0       0.0    0.0        0.0    0.0         0.0      0.0     0.0    0.0   \n",
       "1       0.0    0.0        0.0    0.0         0.0      0.0     0.0    0.0   \n",
       "2       0.0    0.0        0.0    0.0         0.0      0.0     0.0    0.0   \n",
       "3       0.0    0.0        0.0    0.0         0.0      0.0     0.0    0.0   \n",
       "4       0.0    0.0        0.0    0.0         0.0      0.0     0.0    0.0   \n",
       "\n",
       "   zidonians  ziha  ziklag  zimri  zin  zion  ziph  zippor  zoan  zoar  zobah  \\\n",
       "0        0.0   0.0     0.0    0.0  0.0   0.0   0.0     0.0   0.0   0.0    0.0   \n",
       "1        0.0   0.0     0.0    0.0  0.0   0.0   0.0     0.0   0.0   0.0    0.0   \n",
       "2        0.0   0.0     0.0    0.0  0.0   0.0   0.0     0.0   0.0   0.0    0.0   \n",
       "3        0.0   0.0     0.0    0.0  0.0   0.0   0.0     0.0   0.0   0.0    0.0   \n",
       "4        0.0   0.0     0.0    0.0  0.0   0.0   0.0     0.0   0.0   0.0    0.0   \n",
       "\n",
       "   zodiac  zohar  zone  zorah  zuar  zuph  zurishaddai  \n",
       "0     0.0    0.0   0.0    0.0   0.0   0.0          0.0  \n",
       "1     0.0    0.0   0.0    0.0   0.0   0.0          0.0  \n",
       "2     0.0    0.0   0.0    0.0   0.0   0.0          0.0  \n",
       "3     0.0    0.0   0.0    0.0   0.0   0.0          0.0  \n",
       "4     0.0    0.0   0.0    0.0   0.0   0.0          0.0  \n",
       "\n",
       "[5 rows x 14011 columns]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Updated to move the train/test code before the TFIDVectorizer \n",
    "from sklearn.model_selection import train_test_split\n",
    "#splitting into training and test sets. Reserving 25% of my corpus as a test set.\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X, \n",
    "                                                                Y,\n",
    "                                                                test_size=0.25,\n",
    "                                                                random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "#Generate features using TFIDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=3, # only use words that appear at least twice\n",
    "                             stop_words=stopwords, \n",
    "                             lowercase=True, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "#tfidf=vectorizer.fit_transform is using the training data\n",
    "tfidf=vectorizer.fit_transform(X_train_tfidf) #training a model when using a model\n",
    "terms = vectorizer.get_feature_names()\n",
    "X_tfidf = pd.DataFrame(data=tfidf.toarray(),\n",
    "             columns=terms)\n",
    "\n",
    "tfidf_test=vectorizer.transform(X_test_tfidf) #use this to test the model\n",
    "\n",
    "X_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Actus Primus Scoena Prima Enter Flauius Murell...\n",
       "2    replication of your sound Made in her Concaue ...\n",
       "3    wing Will make him flye an ordinary pitch Who ...\n",
       "5    Forgets the shewes of Loue to other men Cassi ...\n",
       "6    or did vse To stale with ordinary Oathes my lo...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify the index series. As a result of the test/train split I think the index our \n",
    "#no longer sequential\n",
    "pd.Series.sort_index(X_train_tfidf).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The output above confirms the index are not sequential and this will cause problems for us\n",
    "#during our model clustering so lets reset the index now for all of the test/train variables\n",
    "X_train_tfidf = X_train_tfidf.reset_index(drop=True)\n",
    "X_test_tfidf = X_test_tfidf.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "#X_train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    carriage with four horse and with her own comp...\n",
       "1    The guidon flag flutter gayly in the wind Bivo...\n",
       "2    and Buster Bear had been fishing together in t...\n",
       "3    one stroke I feel like that he said laughing b...\n",
       "4    the cruel order of her father and she said at ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify the index series \n",
    "pd.Series.sort_index(X_train_tfidf).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate clusters for K-means,MeanShift,SpectralClustering and AffinityPropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0: Authour code: 0 whale ahab boat ship ye sea upon stubb captain like\n",
      "Cluster 1: Authour code: 9 unto shall lord thou ye thy god thee son israel\n",
      "Cluster 2: Authour code: 4 browns farmer buster boy berry green bear forest blacky pail\n",
      "Cluster 3: Authour code: 6 love see old soul night thee day shall life thou\n",
      "Cluster 4: Authour code: 3 haue ham caesar thou bru enter lord macb brutus shall\n",
      "Cluster 5: Authour code: 0 mrs anne could elinor elliot would mr lady marianne miss\n",
      "Cluster 6: Authour code: 8 buster joe little bear pool billy otter trout fish mink\n",
      "Cluster 7: Authour code: 7 little jackal margery tailor came lion mother tiger boy nightingale\n",
      "Cluster 8: Authour code: 8 alice queen hatter dormouse turtle cat gryphon mock rabbit dont\n",
      "Cluster 9: Authour code: 2 turnbull syme macian man like brown face seemed thing quite\n",
      "Prediction\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing k-means clusters against authors:\n",
      "col_0           0    1   2    3    4    5   6    7    8    9  Total\n",
      "author_codes                                                       \n",
      "0               0    0   0    9    0  280   0    0    0    0    289\n",
      "1               0  195   0    2    0    0   0    0    0    0    197\n",
      "2               0    0   0   80    1    0   0   16    0    0     97\n",
      "3               2    1   0   71    0    0   0   97    0    0    171\n",
      "4               0    0  58    5    0    0  57    8    0    0    128\n",
      "5               0    0   0   18    0    0   0   16  164    0    198\n",
      "6               0    0   0   17    0    0   0    0    0  280    297\n",
      "7             142    0   0   17    0    0   0    0    0    7    166\n",
      "8               0    0   0    1  255    0   0    0    0    0    256\n",
      "9               0    0   0  218    0    0   0    0    0    0    218\n",
      "Total         144  196  58  438  256  280  57  137  164  287   2017\n"
     ]
    }
   ],
   "source": [
    "#The goal of the following code is to print the K-means clusters and top terms by author source.\n",
    "#The model has a cluster centers attribute that returns the coordinates of each of the \n",
    "#k cluster centroids. Each token in the vectorizer we created earlier has a dimension or \n",
    "#coordinate in the centroid and represents its relative frequency within that cluster. \n",
    "#So to find the words with the highest frequency in a cluster, we have to locate the \n",
    "#indices of the highest values of the centroid, which then correspond to the indices of \n",
    "#the tokens in the vectorizer. Therefore, below are the top ten frequented words in \n",
    "#each cluster for K-Means\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "n_clusters = 10\n",
    "model = KMeans(n_clusters=n_clusters, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(tfidf)\n",
    "pred = model.predict(tfidf) \n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(n_clusters):\n",
    "    print(\"Cluster %s: Authour code: %s\" %(i, np.array(y_train[i])),end='')\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind],end='')\n",
    "    print()\n",
    "    #print(\"\\n\")\n",
    "print(\"Prediction\")\n",
    "\n",
    "plt.hist(model.labels_, bins=n_clusters)\n",
    "plt.xlabel('Clusters')\n",
    "plt.grid(color='b', linestyle='-', linewidth=1)\n",
    "plt.ylabel('Frequency of terms')\n",
    "plt.title('Frequency of terms by cluster')\n",
    "plt.show()\n",
    "\n",
    "print('Comparing k-means clusters against authors:')\n",
    "print(pd.crosstab(y_train, pred, margins=True, margins_name=\"Total\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                           0         1  \\\n",
      "text                                                                     \n",
      "carriage with four horse and with her own compl...  0.491350 -0.535527   \n",
      "The guidon flag flutter gayly in the wind Bivou...  0.661872 -0.367992   \n",
      "and Buster Bear had been fishing together in th...  0.142769 -0.204082   \n",
      "one stroke I feel like that he said laughing bu...  0.568744 -0.595667   \n",
      "the cruel order of her father and she said at o...  0.827759 -0.136761   \n",
      "\n",
      "                                                           2         3  \\\n",
      "text                                                                     \n",
      "carriage with four horse and with her own compl...  0.572407  0.013017   \n",
      "The guidon flag flutter gayly in the wind Bivou... -0.391347 -0.093228   \n",
      "and Buster Bear had been fishing together in th... -0.264529  0.626957   \n",
      "one stroke I feel like that he said laughing bu... -0.371830  0.094773   \n",
      "the cruel order of her father and she said at o...  0.156910  0.071465   \n",
      "\n",
      "                                                           4         5  \\\n",
      "text                                                                     \n",
      "carriage with four horse and with her own compl...  0.072459  0.086950   \n",
      "The guidon flag flutter gayly in the wind Bivou... -0.002168 -0.054611   \n",
      "and Buster Bear had been fishing together in th...  0.550149  0.397275   \n",
      "one stroke I feel like that he said laughing bu... -0.149801 -0.288787   \n",
      "the cruel order of her father and she said at o...  0.121547 -0.105558   \n",
      "\n",
      "                                                           6         7  \\\n",
      "text                                                                     \n",
      "carriage with four horse and with her own compl...  0.021898 -0.089442   \n",
      "The guidon flag flutter gayly in the wind Bivou... -0.272052  0.270843   \n",
      "and Buster Bear had been fishing together in th... -0.081559 -0.082792   \n",
      "one stroke I feel like that he said laughing bu... -0.111153 -0.132994   \n",
      "the cruel order of her father and she said at o...  0.173439  0.104834   \n",
      "\n",
      "                                                           8         9  \n",
      "text                                                                    \n",
      "carriage with four horse and with her own compl... -0.144676 -0.318975  \n",
      "The guidon flag flutter gayly in the wind Bivou... -0.312612  0.128770  \n",
      "and Buster Bear had been fishing together in th...  0.023477  0.019192  \n",
      "one stroke I feel like that he said laughing bu... -0.180136  0.078369  \n",
      "the cruel order of her father and she said at o... -0.327865  0.303151  \n",
      "\n",
      " Explained variance of the SVD step: 9%\n",
      "\n",
      " Comparing training k-means clusters against author codes:\n",
      "col_0           0    1    2    3    4    5    6    7    8   9  Total\n",
      "author_codes                                                        \n",
      "0               0    0    0    0    1  203    0    0    0  85    289\n",
      "1               0    1    0  196    0    0    0    0    0   0    197\n",
      "2               0   74    0    0   16    0    3    0    3   1     97\n",
      "3               0    7    1    3  141    0    4    6    9   0    171\n",
      "4               0    0    0    0    8    0  120    0    0   0    128\n",
      "5               0    1  177    0   20    0    0    0    0   0    198\n",
      "6               0    1    0    0    8    0    0    0  288   0    297\n",
      "7               0    4    0    0   10    0    0  151    1   0    166\n",
      "8             255    0    0    1    0    0    0    0    0   0    256\n",
      "9               0  213    0    0    5    0    0    0    0   0    218\n",
      "Total         255  301  178  200  209  203  127  157  301  86   2017\n",
      "\n",
      " Comparing testing k-means clusters against author codes:\n",
      "col_0          0   1   2   3   4   5   6   7    8   9  Total\n",
      "author_codes                                                \n",
      "0              0   0   0   0   2  72   0   0    0  35    109\n",
      "1              0   0   0  67   0   0   0   0    0   0     67\n",
      "2              0  32   0   0   6   0   1   0    1   0     40\n",
      "3              0   0   0   0  57   0   1   0    2   0     60\n",
      "4              0   0   0   0   1   0  30   0    0   0     31\n",
      "5              0   0  62   0   5   0   0   0    0   1     68\n",
      "6              0   0   0   0   5   0   0   0  107   1    113\n",
      "7              0   5   0   0   2   0   0  41    0   0     48\n",
      "8             83   0   0   0   0   0   0   0    0   0     83\n",
      "9              0  53   0   0   1   0   0   0    0   0     54\n",
      "Total         83  90  62  67  79  72  32  41  110  37    673\n",
      "\n",
      " The adjusted rand score is: 0.8163029382687977\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cluster</th>\n",
       "      <th>Number of clusters</th>\n",
       "      <th>RI adjusted score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>K-Means</td>\n",
       "      <td>10</td>\n",
       "      <td>0.816303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cluster  Number of clusters  RI adjusted score\n",
       "0  K-Means                  10           0.816303"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Running k-means cluster using with PCA to provide a graphically picture of the cluster\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "# Normalize the data to use with PCA.\n",
    "#X_norm = normalize(X_train_tfidf)\n",
    "\n",
    "# Reduce it to two components for visualization only, it is not used in KMeans\n",
    "#X_pca = PCA(2).fit_transform(X_norm)\n",
    "\n",
    "#Our SVD data reducer.  We are going to reduce the feature space from 18880 to 10.\n",
    "svd= TruncatedSVD(10)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(tfidf)\n",
    "X_test_lsa = lsa.transform(tfidf_test)\n",
    "\n",
    "\n",
    "#define variable for components\n",
    "components = svd.components_[1]\n",
    "\n",
    "#print the components of the table\n",
    "lsa_by_component=pd.DataFrame(X_train_lsa,index=X_train_tfidf)\n",
    "#for i in range(10):\n",
    "    #print('Component {}:'.format(i))\n",
    "    #print(lsa_by_component.loc[:,i].sort_values(ascending=False)[0:2])\n",
    "print(lsa_by_component.head())\n",
    "\n",
    "# Calculate predicted values.\n",
    "\n",
    "km = KMeans(n_clusters=10,n_init = 5, n_jobs = -1).fit(X_train_lsa)\n",
    "y_pred = km.predict(X_train_lsa)\n",
    "y_test_pred = km.predict(X_test_lsa)\n",
    "labels = km.labels_\n",
    "\n",
    "#order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "#terms = vectorizer.get_feature_names()\n",
    "#print('The top terms for each Kmeans cluster using LSA: \\n')\n",
    "\n",
    "#if len(order_centroids) == len(svd.components_):\n",
    "#    for i in range(svd.n_components):\n",
    "#        print(\"LSA componet %s %2fs: \" % (i, components[i]), end='')\n",
    "#        print(\"Cluster %s: Author code: %s\" %(i, np.array(y_train[i])),end='')\n",
    "#        for ind in order_centroids[i, :10]:\n",
    "#            print(' %s' % terms[ind], end='')\n",
    "#        print()\n",
    "#else:    \n",
    "#    for i in range(n_clusters_):\n",
    "#        print(\"LSA componet %2f:\" % components[i], end='')\n",
    "#        print(\"Cluster %s: Author code: %s\" %(i, np.array(y_train[i])),end='')\n",
    "#        for ind in order_centroids[i, :10]:\n",
    "#            print(' %s' % terms[ind], end='')\n",
    "#        print()\n",
    "\n",
    "explained_variance = svd.explained_variance_ratio_.sum()\n",
    "print(\"\\n Explained variance of the SVD step: {}%\".format(\n",
    "        int(explained_variance * 100)))\n",
    "\n",
    "#Print out result of pca\n",
    "print('\\n Comparing training k-means clusters against author codes:')\n",
    "print(pd.crosstab(y_train, y_pred, margins=True, margins_name=\"Total\"))\n",
    "\n",
    "print('\\n Comparing testing k-means clusters against author codes:')\n",
    "print(pd.crosstab(y_test, y_test_pred, margins=True, margins_name=\"Total\"))\n",
    "\n",
    "#adjusted rand for k-means\n",
    "RIadjusted = metrics.adjusted_rand_score(y_train, y_pred)\n",
    "print(\"\\n The adjusted rand score is:\",RIadjusted )\n",
    "\n",
    "#create a result table\n",
    "data = [{'Cluster':'K-Means', 'Number of clusters': n_clusters,\n",
    "         'RI adjusted score': RIadjusted}]\n",
    "df_results = pd.DataFrame(data)\n",
    "\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance of the SVD step: 9%\n",
      "The top terms for each MeanShift cluster using LSA: \n",
      "\n",
      "\n",
      " Number of estimated clusters: 10\n",
      "\n",
      " Comparing training meanshift clusters against authors:\n",
      "col_0           0    1    2    3    4    5  Total\n",
      "author_codes                                     \n",
      "0               7    0  282    0    0    0    289\n",
      "1               0    0    0    0  197    0    197\n",
      "2              80    0    0    0   13    4     97\n",
      "3             142    0    2    2    9   16    171\n",
      "4               4    0    0    0    0  124    128\n",
      "5              11    0    0  187    0    0    198\n",
      "6             297    0    0    0    0    0    297\n",
      "7             166    0    0    0    0    0    166\n",
      "8               0  255    0    0    1    0    256\n",
      "9             210    0    0    0    8    0    218\n",
      "Total         917  255  284  189  228  144   2017\n",
      "\n",
      " Comparing testing meanshift clusters against author codes:\n",
      "col_0           0   1    2   3   4   5  Total\n",
      "author_codes                                 \n",
      "0               5   0  104   0   0   0    109\n",
      "1               0   0    0   0  67   0     67\n",
      "2              30   0    0   0   7   3     40\n",
      "3              51   0    0   2   1   6     60\n",
      "4               0   0    0   0   0  31     31\n",
      "5               4   0    0  64   0   0     68\n",
      "6             113   0    0   0   0   0    113\n",
      "7              48   0    0   0   0   0     48\n",
      "8               0  83    0   0   0   0     83\n",
      "9              49   0    0   0   5   0     54\n",
      "Total         300  83  104  66  80  40    673\n",
      "\n",
      " The adjusted rand score is: 0.4618107017232858\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cluster</th>\n",
       "      <th>Number of clusters</th>\n",
       "      <th>RI adjusted score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>K-Means</td>\n",
       "      <td>10</td>\n",
       "      <td>0.816303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MeanShift</td>\n",
       "      <td>10</td>\n",
       "      <td>0.461811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Cluster  Number of clusters  RI adjusted score\n",
       "0    K-Means                  10           0.816303\n",
       "0  MeanShift                  10           0.461811"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mean shift\n",
    "\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "\n",
    "# Here we set the bandwidth. This function automatically derives a bandwidth\n",
    "# number based on an inspection of the distances among points in the data.\n",
    "#need to convert to a dense numpy array so I will use the X_tfidf\n",
    "bandwidth = estimate_bandwidth(X_train_lsa, quantile=0.1, n_samples=X_train_lsa.shape[0])\n",
    "\n",
    "\n",
    "# Declare and fit the model.\n",
    "ms = MeanShift(bandwidth=bandwidth, bin_seeding=True).fit(X_train_lsa)\n",
    "ms_pred = ms.predict(X_train_lsa)\n",
    "y_test_pred = ms.predict(X_test_lsa)\n",
    "#ms.fit(X_pca)\n",
    "    \n",
    "explained_variance = svd.explained_variance_ratio_.sum()\n",
    "print(\"Explained variance of the SVD step: {}%\".format(\n",
    "        int(explained_variance * 100)))\n",
    "\n",
    "# Coordinates of the cluster centers.\n",
    "cluster_centers = ms.cluster_centers_\n",
    "\n",
    "# Count our clusters.\n",
    "n_clusters_ = len(np.unique(labels))\n",
    "\n",
    "# Extract cluster assignments for each data point.\n",
    "labels = ms.labels_\n",
    "\n",
    "ms_order_centroids = ms.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "print('The top terms for each MeanShift cluster using LSA: \\n')\n",
    "\n",
    "#if len(ms_order_centroids) == len(svd.components_):\n",
    "#    for i in range(svd.n_components):\n",
    "#        print(\"LSA componet %s %2fs: \" % (i, components[i]), end='')\n",
    "#        print(\"Cluster %s: Author code: %s\" %(i, np.array(y_train[i])),end='')\n",
    "#        for ind in ms_order_centroids[i, :10]:\n",
    "#            print(' %s' % terms[ind], end='')\n",
    "#        print()\n",
    "#else:    \n",
    "#    for i in range(n_clusters_):\n",
    "#        print(\"LSA componet %s %2fs: \" % (i, components[i]), end='')\n",
    "#        print(\"Cluster %s: Author code: %s\" %(i, np.array(y_train[i])),end='')\n",
    "#        for ind in ms_order_centroids[i, :10]:\n",
    "#            print(' %s' % terms[ind], end='')\n",
    "#        print()\n",
    "\n",
    "print(\"\\n Number of estimated clusters: {}\".format(n_clusters_))\n",
    "#print (\"Labels identified: {}\".format(labels)) # dosn't tell me much as it is just numbers\n",
    "\n",
    "print('\\n Comparing training meanshift clusters against authors:')\n",
    "print(pd.crosstab(y_train, ms_pred, margins=True, margins_name=\"Total\"))\n",
    "\n",
    "print('\\n Comparing testing meanshift clusters against author codes:')\n",
    "print(pd.crosstab(y_test, y_test_pred, margins=True, margins_name=\"Total\"))\n",
    "\n",
    "RIadjusted = metrics.adjusted_rand_score(y_train, ms_pred)\n",
    "print(\"\\n The adjusted rand score is:\",RIadjusted )\n",
    "\n",
    "#update result table\n",
    "data = [{'Cluster':'MeanShift', 'Number of clusters': n_clusters_,\n",
    "         'RI adjusted score': RIadjusted}]\n",
    "\n",
    "d2 = pd.DataFrame(data)\n",
    "df_results = df_results.append(d2) \n",
    "\n",
    "df_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing training spectral clusters against authors:\n",
      "col_0           0    1    2    3    4    5   6    7    8    9  Total\n",
      "author_codes                                                        \n",
      "0             196    8    0    0    0    0  85    0    0    0    289\n",
      "1               0    0    5    0    0   36   0  156    0    0    197\n",
      "2               0   15   79    3    0    0   0    0    0    0     97\n",
      "3               0  112   18    4    1   31   0    0    5    0    171\n",
      "4               0    8    0  120    0    0   0    0    0    0    128\n",
      "5               0   23    1    0  173    1   0    0    0    0    198\n",
      "6               0  296    1    0    0    0   0    0    0    0    297\n",
      "7               0   16    8    0    0    0   0    0  142    0    166\n",
      "8               0    0    5    0    0   62   0    0    0  189    256\n",
      "9               0    5  213    0    0    0   0    0    0    0    218\n",
      "Total         196  483  330  127  174  130  85  156  147  189   2017\n",
      "\n",
      " Comparing testing spectral clusters against author codes:\n",
      "col_0          0   1   2   3   4   5    6   7   8   9  Total\n",
      "author_codes                                                \n",
      "0              0   0  72  35   0   0    2   0   0   0    109\n",
      "1              0   0   0   0   2  65    0   0   0   0     67\n",
      "2             33   1   0   0   0   0    6   0   0   0     40\n",
      "3              2   1   0   0   3   0   52   1   1   0     60\n",
      "4              0  29   0   0   0   0    2   0   0   0     31\n",
      "5              0   0   0   0   0   0    6   0  62   0     68\n",
      "6              0   0   0   0   0   0  113   0   0   0    113\n",
      "7              6   0   0   0   0   0    3  39   0   0     48\n",
      "8              3   0   0   0  34   0    0   0   0  46     83\n",
      "9             50   0   0   0   0   0    4   0   0   0     54\n",
      "Total         94  31  72  35  39  65  188  40  63  46    673\n",
      "\n",
      " The adjusted rand score is: 0.6286619974749877\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cluster</th>\n",
       "      <th>Number of clusters</th>\n",
       "      <th>RI adjusted score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>K-Means</td>\n",
       "      <td>10</td>\n",
       "      <td>0.816303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MeanShift</td>\n",
       "      <td>10</td>\n",
       "      <td>0.461811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SpectralClustering</td>\n",
       "      <td>10</td>\n",
       "      <td>0.628662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Cluster  Number of clusters  RI adjusted score\n",
       "0             K-Means                  10           0.816303\n",
       "0           MeanShift                  10           0.461811\n",
       "0  SpectralClustering                  10           0.628662"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#apectral clustering\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "# We know we're looking for three clusters.\n",
    "n_clusters=10\n",
    "\n",
    "# Declare and fit the model. Spectral clustering dosn't have a .predict method only\n",
    "#so I must use the fit_predict method\n",
    "sc = SpectralClustering(n_clusters=n_clusters)\n",
    "#sc.fit(tfidf)\n",
    "sc_pred = sc.fit_predict(X_train_lsa)\n",
    "y_test_pred = sc.fit_predict(X_test_lsa)\n",
    "\n",
    "#can't print the cluster centers so I am unable show the LSA components\n",
    "\n",
    "print('Comparing training spectral clusters against authors:')\n",
    "print(pd.crosstab(y_train, sc_pred, margins=True, margins_name=\"Total\"))\n",
    "\n",
    "print('\\n Comparing testing spectral clusters against author codes:')\n",
    "print(pd.crosstab(y_test, y_test_pred, margins=True, margins_name=\"Total\"))\n",
    "\n",
    "RIadjusted = metrics.adjusted_rand_score(y_train, sc_pred)\n",
    "print(\"\\n The adjusted rand score is:\",RIadjusted )\n",
    "\n",
    "#update result table\n",
    "data = [{'Cluster':'SpectralClustering', 'Number of clusters': n_clusters,\n",
    "         'RI adjusted score': RIadjusted}]\n",
    "\n",
    "d2 = pd.DataFrame(data)\n",
    "df_results = df_results.append(d2) \n",
    "\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top terms for each Affinity cluster using LSA: \n",
      "\n",
      "Comparing training affinity clusters against authors:\n",
      "col_0          0   1   2   3   4   5   6   7   8    9  10  11  12  13  14  15  \\\n",
      "author_codes                                                                    \n",
      "0              0   0   0   0   0   0   0   0  68    0   0   0   0   0   0   0   \n",
      "1             47   0   0   0   0  36   0   0   0    0   0   0  45   0  37   0   \n",
      "2              0   8   2   0  17   0   0   0   0    0   0   5   0   0   0   0   \n",
      "3              0  16  44  26   0   0   0   0   0    0   0   0   0   1   3   0   \n",
      "4              0   0   4   0   0   0   0   0   0    0   0   0   0   0   0   0   \n",
      "5              0   0  11   2   0   0   0   0   0  105   0   0   0  59   0   0   \n",
      "6              0   0   0   0   0   0   0  91   0    0   0   0   0   0   0   0   \n",
      "7              0   0   0   0   0   0   0   0   0    0   0   0   0   0   0  45   \n",
      "8              0   0   0   0   0   0  40   0   0    0  43   0   0   0   1   0   \n",
      "9              0   2   0   0   8   0   0   0   0    0   0   3   0   0   0   0   \n",
      "Total         47  26  61  28  25  36  40  91  68  105  43   8  45  60  41  45   \n",
      "\n",
      "col_0         16   17  18  19  20   21  22  23  24  25  26  27  28  29  30  \\\n",
      "author_codes                                                                 \n",
      "0              0    0   0  64   0    0   1   0   0   0   0  70   0   0   0   \n",
      "1              0    0   0   0   0    0   0   0   0   0   0   0   0   0   0   \n",
      "2              0    0  16   0   8    0   0   3   2   0  14   0   0   0   9   \n",
      "3              2    6   3   0   0    0   1   7   2   0  12   0  29   0   1   \n",
      "4              0    0   0   0   0  107   0  17   0   0   0   0   0   0   0   \n",
      "5              0    0   0   0   0    0   1   0   0   0   0   0   0  18   0   \n",
      "6              1  135   0   0   0    0  56   0   0   0   0   0   0   0   1   \n",
      "7             43    0   0   0   1    0   0   0   0   0   0   0   0   0   1   \n",
      "8              0    0   0   0   0    0   0   0   0  38   0   0   0   0   0   \n",
      "9              1    0  13   0  44    0   0   0  37   0   1   0   0   0  75   \n",
      "Total         47  141  32  64  53  107  59  27  41  38  27  70  29  18  87   \n",
      "\n",
      "col_0         31  32  33  34  35  36  37  Total  \n",
      "author_codes                                     \n",
      "0              0   0   1  85   0   0   0    289  \n",
      "1             32   0   0   0   0   0   0    197  \n",
      "2              0   4   9   0   0   0   0     97  \n",
      "3              0  13   2   0   0   0   3    171  \n",
      "4              0   0   0   0   0   0   0    128  \n",
      "5              0   1   1   0   0   0   0    198  \n",
      "6              0  10   3   0   0   0   0    297  \n",
      "7              0   9   0   0   0   0  67    166  \n",
      "8              0   0   0   0  71  63   0    256  \n",
      "9              0   1  33   0   0   0   0    218  \n",
      "Total         32  38  49  85  71  63  70   2017  \n",
      "Estimated number of clusters: 22\n",
      "\n",
      " Comparing testing affinity clusters against author codes:\n",
      "col_0          0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  \\\n",
      "author_codes                                                                   \n",
      "0             46   0   0   0   0   0   0   0   0  26   0   0  35   0   0   0   \n",
      "1              0   0  25   0   0   0   0   0  29   0   0   0   0   0   0   0   \n",
      "2              0   0   0   1  14   0   0   8   0   0   0   9   0   2   0   0   \n",
      "3              0   0   1   1   1   0   0   0   0   0   0   0   0   9   0   0   \n",
      "4              0   0   0  30   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "5              0   0   0   0   0   0   0   0   0   0  41   0   0   0  22   0   \n",
      "6              0  55   0   0   0   0   0   0   0   0   0   0   1   1   0   0   \n",
      "7              0   0   0   0   0   0   0   0   0   0   0   3   0   0   0  40   \n",
      "8              0   0   0   0   0  34  17   0   0   0   0   0   0   0   0   0   \n",
      "9              0   0   0   0  21   0   0   5   0   0   0  15   0   0   0   0   \n",
      "Total         46  55  26  32  36  34  17  13  29  26  41  27  36  12  22  40   \n",
      "\n",
      "col_0         16  17  18  19  20  21  Total  \n",
      "author_codes                                 \n",
      "0              0   2   0   0   0   0    109  \n",
      "1              0   0   0   0   0  13     67  \n",
      "2              1   4   1   0   0   0     40  \n",
      "3             16   4  23   0   5   0     60  \n",
      "4              0   0   1   0   0   0     31  \n",
      "5              3   0   1   0   1   0     68  \n",
      "6              2   1   0   0  53   0    113  \n",
      "7              1   4   0   0   0   0     48  \n",
      "8              0   0   0  32   0   0     83  \n",
      "9              0  13   0   0   0   0     54  \n",
      "Total         23  28  26  32  59  13    673  \n",
      "Estimated number of clusters: 22\n",
      "\n",
      " The adjusted rand score is: 0.3891680526189438\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cluster</th>\n",
       "      <th>Number of clusters</th>\n",
       "      <th>RI adjusted score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>K-Means</td>\n",
       "      <td>10</td>\n",
       "      <td>0.816303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MeanShift</td>\n",
       "      <td>10</td>\n",
       "      <td>0.461811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SpectralClustering</td>\n",
       "      <td>10</td>\n",
       "      <td>0.628662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Affinity Clustering</td>\n",
       "      <td>22</td>\n",
       "      <td>0.389168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Cluster  Number of clusters  RI adjusted score\n",
       "0              K-Means                  10           0.816303\n",
       "0            MeanShift                  10           0.461811\n",
       "0   SpectralClustering                  10           0.628662\n",
       "0  Affinity Clustering                  22           0.389168"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn import metrics\n",
    "\n",
    "# Declare the model and fit it in one statement.\n",
    "# Note that you can provide arguments to the model, but we didn't.\n",
    "af = AffinityPropagation().fit(X_train_lsa)\n",
    "af_pred = af.fit_predict(X_train_lsa)\n",
    "y_test_pred = af.fit_predict(X_test_lsa)\n",
    "\n",
    "\n",
    "# Pull the number of clusters and cluster assignments for each data point.\n",
    "cluster_centers_indices = af.cluster_centers_indices_\n",
    "n_clusters_ = len(cluster_centers_indices)\n",
    "labels = af.labels_\n",
    "\n",
    "af_order_centroids = af.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "print('The top terms for each Affinity cluster using LSA: \\n')\n",
    "\n",
    "#if len(af_order_centroids) == len(svd.components_):\n",
    "#    for i in range(svd.n_components):\n",
    "#        print(\"LSA componet %s %2fs: \" % (i, components[i]), end='')\n",
    "#        print(\"Cluster %s: Author code: %s\" %(i, np.array(y_train[i])),end='')\n",
    "#        for ind in af_order_centroids[i, :10]:\n",
    "#            print(' %s' % terms[ind], end='')\n",
    "#        print()\n",
    "#else:    \n",
    "#    for i in range(n_clusters_):\n",
    "#        print(\"LSA componet %s %2fs: \" % (i, components[i]), end='')\n",
    "#        print(\"Cluster %s: Author code: %s\" %(i, np.array(y_train[i])),end='')\n",
    "#        for ind in af_order_centroids[i, :10]:\n",
    "#            print(' %s' % terms[ind], end='')\n",
    "#        print()\n",
    "\n",
    "\n",
    "print('Comparing training affinity clusters against authors:')\n",
    "print(pd.crosstab(y_train, af_pred, margins=True, margins_name=\"Total\"))\n",
    "print('Estimated number of clusters: {}'.format(n_clusters_))\n",
    "\n",
    "print('\\n Comparing testing affinity clusters against author codes:')\n",
    "print(pd.crosstab(y_test, y_test_pred, margins=True, margins_name=\"Total\"))\n",
    "print('Estimated number of clusters: {}'.format(n_clusters_))\n",
    "\n",
    "RIadjusted = metrics.adjusted_rand_score(y_train, af_pred)\n",
    "print(\"\\n The adjusted rand score is:\",RIadjusted )\n",
    "\n",
    "#update result table\n",
    "data3 = [{'Cluster':'Affinity Clustering', 'Number of clusters': n_clusters_,\n",
    "         'RI adjusted score': RIadjusted}]\n",
    "\n",
    "\n",
    "#update result table\n",
    "#data = [{'Cluster':'AffinityPropagation', 'Number of clusters': len(cluster_centers_indices),\n",
    "#         'RI adjusted score': RIadjusted}]\n",
    "\n",
    "d3 = pd.DataFrame(data3)\n",
    "df_results = df_results.append(d) \n",
    "\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the accurcy for each of the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cluster</th>\n",
       "      <th>Number of clusters</th>\n",
       "      <th>RI adjusted score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>K-Means</td>\n",
       "      <td>10</td>\n",
       "      <td>0.816303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MeanShift</td>\n",
       "      <td>10</td>\n",
       "      <td>0.461811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SpectralClustering</td>\n",
       "      <td>10</td>\n",
       "      <td>0.628662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Affinity Clustering</td>\n",
       "      <td>22</td>\n",
       "      <td>0.389168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Cluster  Number of clusters  RI adjusted score\n",
       "0              K-Means                  10           0.816303\n",
       "1            MeanShift                  10           0.461811\n",
       "2   SpectralClustering                  10           0.628662\n",
       "3  Affinity Clustering                  22           0.389168"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reset the index so that I can update the RI and adjusted score later.\n",
    "df_results = df_results.reset_index(drop=True)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RIscore is: 0.020440384362171368\n",
      "The adjusted rand score is: 0.7211034660272139\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cluster</th>\n",
       "      <th>Number of clusters</th>\n",
       "      <th>RI Score</th>\n",
       "      <th>RI adjusted score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>K-Means</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0204404</td>\n",
       "      <td>0.721103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SpectralClustering</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0204404</td>\n",
       "      <td>0.721103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MeanShift</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0204404</td>\n",
       "      <td>0.721103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AffinityPropagation</td>\n",
       "      <td>24</td>\n",
       "      <td>0.0204404</td>\n",
       "      <td>0.721103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Cluster  Number of clusters   RI Score  RI adjusted score\n",
       "0              K-Means                  10  0.0204404           0.721103\n",
       "0   SpectralClustering                  10  0.0204404           0.721103\n",
       "0            MeanShift                   6  0.0204404           0.721103\n",
       "0  AffinityPropagation                  24  0.0204404           0.721103"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In summation based on my results table none of my clusters are predicting a 100% agreement between my ground truth and my solution but Kmeans Clustering is predicting the higher RI adjusted score and implies it is predicting the most accurate number of clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Training mean set score: 0.9260875957840009\n",
      "KNN Testing mean set score: 0.876811848551819\n",
      "\n",
      " KNN Confustion Matrix \n",
      " [[108   0   0   0   0   0   0   1   0   0]\n",
      " [  0  67   0   0   0   0   0   0   0   0]\n",
      " [  0  11  26   1   1   0   0   0   0   1]\n",
      " [  0   5   0  51   0   4   0   0   0   0]\n",
      " [  0   0   0   0  31   0   0   0   0   0]\n",
      " [  0   0   1   0   1  66   0   0   0   0]\n",
      " [  1   0   0   0   0   0 111   1   0   0]\n",
      " [  0   1   0   0   0   0   0  47   0   0]\n",
      " [  0   5   0   0   0   0   0   0  78   0]\n",
      " [  0  14   0   0   0   0   1   0   0  39]]\n",
      "\n",
      " KNN Classification Report \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       109\n",
      "           1       0.65      1.00      0.79        67\n",
      "           2       0.96      0.65      0.78        40\n",
      "           3       0.98      0.85      0.91        60\n",
      "           4       0.94      1.00      0.97        31\n",
      "           5       0.94      0.97      0.96        68\n",
      "           6       0.99      0.98      0.99       113\n",
      "           7       0.96      0.98      0.97        48\n",
      "           8       1.00      0.94      0.97        83\n",
      "           9       0.97      0.72      0.83        54\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       673\n",
      "   macro avg       0.94      0.91      0.91       673\n",
      "weighted avg       0.94      0.93      0.93       673\n",
      "\n",
      "KNN accuracy score: 0.9271916790490342\n"
     ]
    }
   ],
   "source": [
    "#instantiate the estimator (other names can be clf or and look for the 1 nearest neighbor)\n",
    "#find accuracy on y_test and y_predictions\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import ensemble\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "train = knn.fit(tfidf, y_train)\n",
    "y_pred = knn.predict(tfidf_test)\n",
    "\n",
    "#compare the training and testing results using cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cv_results_train = cross_val_score(knn, tfidf, y_train, cv=5)\n",
    "cv_results_test = cross_val_score(knn, tfidf_test, y_test, cv=5)\n",
    "print('KNN Training mean set score:', cv_results_train.mean())\n",
    "print('KNN Testing mean set score:', cv_results_test.mean())\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print('\\n KNN Confustion Matrix \\n', confusion_matrix(y_test,y_pred))  \n",
    "print('\\n KNN Classification Report \\n', classification_report(y_test,y_pred))  \n",
    "print('KNN accuracy score:', accuracy_score(y_test, y_pred))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC Training mean set score: 0.9930905423072055\n",
      "SVC Testing mean set score: 0.9776786951295511\n",
      "\n",
      " Support vector cufusion matrix \n",
      " [[109   0   0   0   0   0   0   0   0   0]\n",
      " [  0  67   0   0   0   0   0   0   0   0]\n",
      " [  0   0  38   2   0   0   0   0   0   0]\n",
      " [  0   0   0  60   0   0   0   0   0   0]\n",
      " [  0   0   0   0  31   0   0   0   0   0]\n",
      " [  0   0   1   0   0  67   0   0   0   0]\n",
      " [  0   0   0   0   0   0 113   0   0   0]\n",
      " [  0   0   0   0   0   0   0  48   0   0]\n",
      " [  0   0   0   0   0   0   0   0  83   0]\n",
      " [  0   0   0   0   0   0   0   0   0  54]]\n",
      "\n",
      " Support vector classification report \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       109\n",
      "           1       1.00      1.00      1.00        67\n",
      "           2       0.97      0.95      0.96        40\n",
      "           3       0.97      1.00      0.98        60\n",
      "           4       1.00      1.00      1.00        31\n",
      "           5       1.00      0.99      0.99        68\n",
      "           6       1.00      1.00      1.00       113\n",
      "           7       1.00      1.00      1.00        48\n",
      "           8       1.00      1.00      1.00        83\n",
      "           9       1.00      1.00      1.00        54\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       673\n",
      "   macro avg       0.99      0.99      0.99       673\n",
      "weighted avg       1.00      1.00      1.00       673\n",
      "\n",
      "Support vector accuracy score: 0.9955423476968797\n"
     ]
    }
   ],
   "source": [
    "#instantiate the estimator (other names can be clf or and look for the 1 nearest neighbor)\n",
    "#find accuracy on y_test and y_predictions\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import ensemble\n",
    "\n",
    "svc = LinearSVC()\n",
    "train = svc.fit(tfidf, y_train)\n",
    "y_pred = svc.predict(tfidf_test)\n",
    "\n",
    "#compare the training and testing results using cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cv_results_train = cross_val_score(svc, tfidf, y_train, cv=5)\n",
    "cv_results_test = cross_val_score(svc, tfidf_test, y_test, cv=5)\n",
    "print('SVC Training mean set score:', cv_results_train.mean())\n",
    "print('SVC Testing mean set score:', cv_results_test.mean())\n",
    "\n",
    "print('\\n Support vector cufusion matrix \\n',confusion_matrix(y_test,y_pred))  \n",
    "print('\\n Support vector classification report \\n',classification_report(y_test,y_pred))  \n",
    "print('Support vector accuracy score:',accuracy_score(y_test, y_pred))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFC Training mean set score: 0.8924398951302331\n",
      "RFC Testing mean set score: 0.8301027409110038\n",
      "\n",
      " Random Forest confusion matrix \n",
      " [[110   0   1   2   0   0   0   0   0   0]\n",
      " [  0  69   0   0   0   0   0   0   0   0]\n",
      " [  0   0  32   3   0   0   0   0   0   2]\n",
      " [  3   0   8  46   2   5   2   0   2   1]\n",
      " [  0   0   0   0  21   0   0   0   0   0]\n",
      " [  0   0   2   2   0  60   1   0   0   0]\n",
      " [  5   0   0   6   0   4  95   0   0   1]\n",
      " [  1   0   0   1   0   0   1  47   0   0]\n",
      " [  0   0   0   0   0   0   0   0  81   0]\n",
      " [  0   0   3   4   0   1   5   0   0  47]]\n",
      "\n",
      " Random Forest classification report \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.97      0.95       113\n",
      "           1       1.00      1.00      1.00        69\n",
      "           2       0.70      0.86      0.77        37\n",
      "           3       0.72      0.67      0.69        69\n",
      "           4       0.91      1.00      0.95        21\n",
      "           5       0.86      0.92      0.89        65\n",
      "           6       0.91      0.86      0.88       111\n",
      "           7       1.00      0.94      0.97        50\n",
      "           8       0.98      1.00      0.99        81\n",
      "           9       0.92      0.78      0.85        60\n",
      "\n",
      "   micro avg       0.90      0.90      0.90       676\n",
      "   macro avg       0.89      0.90      0.89       676\n",
      "weighted avg       0.90      0.90      0.90       676\n",
      "\n",
      "Random Forest accuracy score: 0.8994082840236687\n"
     ]
    }
   ],
   "source": [
    "#Execute the RandomForestClassifier Model\n",
    "#remove the warnings related to default parameters\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "train = rfc.fit(tfidf, y_train)\n",
    "y_pred = rfc.predict(tfidf_test)\n",
    "\n",
    "#compare the training and testing results using cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cv_results_train = cross_val_score(rfc,tfidf, y_train, cv=5)\n",
    "cv_results_test = cross_val_score(rfc, tfidf_test, y_test, cv=5)\n",
    "print('RFC Training mean set score:', cv_results_train.mean())\n",
    "print('RFC Testing mean set score:', cv_results_test.mean())\n",
    "\n",
    "print('\\n Random Forest confusion matrix \\n',confusion_matrix(y_test,y_pred))  \n",
    "print('\\n Random Forest classification report \\n',classification_report(y_test,y_pred))  \n",
    "print('Random Forest accuracy score:',accuracy_score(y_test, y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Training mean set score: 0.9707663581669694\n",
      "LR Testing mean set score: 0.8975607072562287\n",
      "\n",
      " Logistic regression confusion matrix \n",
      " [[109   0   0   0   0   0   0   0   0   0]\n",
      " [  0  67   0   0   0   0   0   0   0   0]\n",
      " [  0   0  29   3   1   0   0   0   0   7]\n",
      " [  0   0   0  59   0   0   1   0   0   0]\n",
      " [  0   0   0   0  31   0   0   0   0   0]\n",
      " [  1   0   0   1   0  66   0   0   0   0]\n",
      " [  0   0   0   0   0   0 113   0   0   0]\n",
      " [  0   0   0   0   0   0   0  48   0   0]\n",
      " [  0   0   0   0   0   0   0   0  83   0]\n",
      " [  0   0   0   0   0   0   0   0   0  54]]\n",
      "\n",
      " Logistic classification report \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00       109\n",
      "           1       1.00      1.00      1.00        67\n",
      "           2       1.00      0.72      0.84        40\n",
      "           3       0.94      0.98      0.96        60\n",
      "           4       0.97      1.00      0.98        31\n",
      "           5       1.00      0.97      0.99        68\n",
      "           6       0.99      1.00      1.00       113\n",
      "           7       1.00      1.00      1.00        48\n",
      "           8       1.00      1.00      1.00        83\n",
      "           9       0.89      1.00      0.94        54\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       673\n",
      "   macro avg       0.98      0.97      0.97       673\n",
      "weighted avg       0.98      0.98      0.98       673\n",
      "\n",
      "Logistic accuracy score:  0.9791976225854383\n"
     ]
    }
   ],
   "source": [
    "#Execute the LogisticRegression Model\n",
    "#remove the warnings related to default parameters\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "train = lr.fit(tfidf, y_train)\n",
    "y_pred = lr.predict(tfidf_test)\n",
    "\n",
    "#compare the training and testing results using cross validation\n",
    "#remove the warnings related to default parameters\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cv_results_train = cross_val_score(lr, tfidf, y_train, cv=5)\n",
    "cv_results_test = cross_val_score(lr, tfidf_test, y_test, cv=5)\n",
    "print('LR Training mean set score:', cv_results_train.mean())\n",
    "print('LR Testing mean set score:', cv_results_test.mean())\n",
    "\n",
    "print('\\n Logistic regression confusion matrix \\n',confusion_matrix(y_test,y_pred))  \n",
    "print('\\n Logistic classification report \\n',classification_report(y_test,y_pred))  \n",
    "print('Logistic accuracy score: ',accuracy_score(y_test, y_pred)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Training mean set score: 0.9422917168977664\n",
      "Gradient Testing mean set score: 0.8963054988206736\n",
      "\n",
      " Gradient Boosting confusion matrix \n",
      " [[107   0   0   5   0   0   0   0   0   1]\n",
      " [  0  69   0   0   0   0   0   0   0   0]\n",
      " [  0   0  33   3   0   0   0   0   0   1]\n",
      " [  0   1   3  64   0   0   0   0   0   1]\n",
      " [  0   0   0   0  21   0   0   0   0   0]\n",
      " [  0   0   2   4   0  59   0   0   0   0]\n",
      " [  0   0   0   1   0   0 110   0   0   0]\n",
      " [  0   0   0   0   0   0   0  50   0   0]\n",
      " [  0   0   0   3   0   0   0   0  78   0]\n",
      " [  0   0   0   1   1   1   0   0   0  57]]\n",
      "\n",
      " Gradient Boosting classification report \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97       113\n",
      "           1       0.99      1.00      0.99        69\n",
      "           2       0.87      0.89      0.88        37\n",
      "           3       0.79      0.93      0.85        69\n",
      "           4       0.95      1.00      0.98        21\n",
      "           5       0.98      0.91      0.94        65\n",
      "           6       1.00      0.99      1.00       111\n",
      "           7       1.00      1.00      1.00        50\n",
      "           8       1.00      0.96      0.98        81\n",
      "           9       0.95      0.95      0.95        60\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       676\n",
      "   macro avg       0.95      0.96      0.95       676\n",
      "weighted avg       0.96      0.96      0.96       676\n",
      "\n",
      "Gradient Boosting accuracy score: 0.9585798816568047\n"
     ]
    }
   ],
   "source": [
    "#Execute the GradientBoostingClassifier Model\n",
    "clf = ensemble.GradientBoostingClassifier()\n",
    "train = clf.fit(tfidf, y_train)\n",
    "y_pred = clf.predict(tfidf_test)\n",
    "\n",
    "#compare the training and testing results using cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cv_results_train = cross_val_score(clf, tfidf, y_train, cv=5)\n",
    "cv_results_test = cross_val_score(clf, tfidf_test, y_test, cv=5)\n",
    "print('Gradient Training mean set score:', cv_results_train.mean())\n",
    "print('Gradient Testing mean set score:', cv_results_test.mean())\n",
    "\n",
    "print('\\n Gradient Boosting confusion matrix \\n',confusion_matrix(y_test,y_pred))  \n",
    "print('\\n Gradient Boosting classification report \\n',classification_report(y_test,y_pred))  \n",
    "print('Gradient Boosting accuracy score:',accuracy_score(y_test, y_pred)) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Model performance:\n",
    "# KNN and SVM where consistent with each other with as their accuracy scores range from 98 and 99%\n",
    "# Logistic regression, Random forest and Gradient boosting were not consistent as their accuracy scores range from 80, 92 and 96%\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
